{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f72858c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pickle\n",
    "import itertools\n",
    "from shapely.geometry import Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.patches import Polygon as MplPolygon\n",
    "import geopandas as gpd\n",
    "import chronnet_utils as cu \n",
    "import warnings\n",
    "from scipy.linalg import eigvals\n",
    "from scipy import optimize\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from datetime import datetime\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "from sis_steady_state_and_eval import jsd_from_samples,nimfa_sis_steady_state_root_1,recognition_quality\n",
    "\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "from sklearn.metrics import ndcg_score, average_precision_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from compute_y_true import compute_y_true_metrics as compute_y_true\n",
    "current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f893e60d",
   "metadata": {},
   "source": [
    "# Build Chronnet for each grid size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eecfc4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_size : range from 2000 to 7000\n",
    "\n",
    "\n",
    "min_weight =2\n",
    "dmax_scale=2\n",
    "grid_sizes=[1000,2000, 2375, 2750, 3000, 4000, 5000, 6000, 7000]\n",
    "dmax_vals = [x * dmax_scale * np.sqrt(3) for x in grid_sizes]\n",
    "time_bin= '12h'  # time bin for grouping fire events\n",
    "\n",
    "shapefile_path = \"/Users/mabelhu/Desktop/Code/DL_FIRE_SV-C2_576237/fire_archive_SV-C2_576237.shp\"\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "gdf = gdf[gdf['CONFIDENCE'].isin(['h', 'n'])]\n",
    "# transform to meter-based\n",
    "if gdf.crs.to_string() == 'EPSG:4326':\n",
    "    gdf = gdf.to_crs(epsg=3857)\n",
    "# convert to datetime\n",
    "gdf['ACQ_TIME'] = gdf['ACQ_TIME'].astype(str).str.zfill(4)\n",
    "gdf['acq_time'] = pd.to_datetime(\n",
    "    gdf['ACQ_DATE'].astype(str) + ' ' + gdf['ACQ_TIME'],\n",
    "    format='%Y-%m-%d %H%M'\n",
    ")\n",
    "gdf['acq_time'] = gdf['acq_time'].apply(pd.Timestamp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "chronnet_graph_list =[]\n",
    "chronnet_result_list = []\n",
    "\n",
    "for hex_size, d_max in zip(grid_sizes, dmax_vals):\n",
    "\n",
    "    hex_grid = cu.create_hex_grid(gdf,hex_size)\n",
    "    gdf_sjoined = gpd.sjoin(\n",
    "        gdf,\n",
    "        hex_grid[['cell', 'geometry']],\n",
    "        how='left',\n",
    "        predicate='within',\n",
    "        rsuffix='_hex'\n",
    "    ).dropna(subset=['cell'])\n",
    "\n",
    "    gdf_sjoined['time_group'] = gdf_sjoined['acq_time'].dt.floor(time_bin)\n",
    "\n",
    "    valid_cells = gdf_sjoined['cell'].unique()\n",
    "    hex_grid_filtered = hex_grid[hex_grid['cell'].isin(valid_cells)]\n",
    "\n",
    "    chronnet = cu.build_chronnet_freq(gdf_sjoined, dmax=d_max, freq=time_bin)\n",
    "    chronnet_pruned = cu.prune_chronnet(chronnet, min_weight=min_weight)\n",
    "\n",
    "\n",
    "    # decribe the network  \n",
    "    total_nodes_before_pruning = chronnet.number_of_nodes()\n",
    "    total_nodes = chronnet_pruned.number_of_nodes()\n",
    "    total_edges_before_pruning = chronnet.number_of_edges()\n",
    "    total_edges = chronnet_pruned.number_of_edges()\n",
    "    total_strength = sum(dict(chronnet_pruned.degree(weight='weight')).values())\n",
    "    total_strength_before_pruning = sum(dict(chronnet.degree(weight='weight')).values())\n",
    "\n",
    "\n",
    "    # compute the largest strongly connected component (SCC)\n",
    "    scc = sorted(nx.strongly_connected_components(chronnet_pruned), key=len, reverse=True)\n",
    "    if len(scc) > 0:\n",
    "        largest_scc = scc[0]\n",
    "        largest_scc_nodes = len(largest_scc)\n",
    "        largest_scc_ratio = len(largest_scc) / total_nodes\n",
    "    else:\n",
    "        largest_cc_nodes = 0\n",
    "        largest_scc_ratio =0\n",
    "   \n",
    "   # compute network-driven fire %\n",
    "\n",
    "    df_fire = gdf_sjoined[gdf_sjoined['cell'].isin(valid_cells)].copy()\n",
    "    df_fire = df_fire[df_fire['cell'].isin(chronnet_pruned.nodes())] # remove cell not in chronnet_pruned\n",
    "\n",
    "\n",
    "    burning_by_time = df_fire.groupby('time_group')['cell'].apply(set).to_dict()\n",
    "    network_events = []\n",
    "    sf_events = []\n",
    "    self_loops_events = []\n",
    "    for t, cells in burning_by_time.items():\n",
    "        prev_cells = burning_by_time.get(t - pd.Timedelta(hours=12), set())\n",
    "        for node in cells:\n",
    "            neis = set(chronnet_pruned.neighbors(node))\n",
    "            if node in prev_cells:\n",
    "                self_loops_events.append((node, t))\n",
    "            elif not prev_cells.intersection(neis):\n",
    "                sf_events.append((node, t))\n",
    "            else:\n",
    "                network_events.append((node, t))\n",
    "\n",
    "    df_network_fire = pd.DataFrame(network_events, columns=['cell','time_group']).drop_duplicates()\n",
    "    grouped_df_fire_unique = df_fire[['cell','time_group']].drop_duplicates()\n",
    "    df_sf_fire = pd.DataFrame(sf_events, columns=['cell','time_group']).drop_duplicates() \n",
    "    df_self_loops = pd.DataFrame(self_loops_events, columns=['cell','time_group']).drop_duplicates()\n",
    "\n",
    "\n",
    "    # create a subset of df_network_fire with cells in largest_scc\n",
    "    df_network_fire['in_largest_scc'] = df_network_fire['cell'].isin(largest_scc)\n",
    "    df_sf_fire['in_largest_scc'] = df_sf_fire['cell'].isin(largest_scc)\n",
    "    df_self_loops['in_largest_scc'] = df_self_loops['cell'].isin(largest_scc)\n",
    "\n",
    "\n",
    "    \n",
    "    chronnet_graph_list.append({\n",
    "        'grid_size': hex_size,\n",
    "        'd_max': d_max,\n",
    "        'chronnet': chronnet,\n",
    "        'chronnet_pruned': chronnet_pruned\n",
    "    \n",
    "    })\n",
    "\n",
    "    chronnet_result_list.append({\n",
    "        'grid_size': hex_size,\n",
    "        'd_max': d_max,\n",
    "        'gdf_sjoined': gdf_sjoined,\n",
    "        'total_nodes_after_pruning': total_nodes,\n",
    "        'total_nodes_before_pruning': total_nodes_before_pruning,\n",
    "        'total_edges_after_pruning': total_edges,\n",
    "        'total_edges_before_pruning': total_edges_before_pruning,\n",
    "        'total_strength_after_pruning': total_strength,\n",
    "        'total_strength_before_pruning': total_strength_before_pruning,\n",
    "\n",
    "\n",
    "        'largest_scc_nodes': largest_scc_nodes,\n",
    "        'largest_scc': largest_scc,\n",
    "        'largest_scc_ratio': largest_scc_ratio,\n",
    "\n",
    "        'grouped_df_fire_unique': grouped_df_fire_unique,\n",
    "        'df_network_fire': df_network_fire,\n",
    "        'df_sf_fire': df_sf_fire,\n",
    "        'df_self_loops': df_self_loops,\n",
    "\n",
    "\n",
    "        'n_all_fire-events': len(network_events) + len(sf_events) + len(self_loops_events),\n",
    "        'n_network_events': len(network_events),\n",
    "        'n_sf_events': len(sf_events),\n",
    "        'n_self_loops_events': len(self_loops_events),\n",
    "\n",
    "        'n_network_fire': len(df_network_fire),\n",
    "        'n_sf_fire': len(df_sf_fire),\n",
    "        'n_self_loops': len(df_self_loops),\n",
    "\n",
    "        'n_network_fire_in_largest_scc': len(df_network_fire[df_network_fire['in_largest_scc']]),\n",
    "        'n_sf_fire_in_largest_scc': len(df_sf_fire[df_sf_fire['in_largest_scc']]),\n",
    "        'n_self_loops_in_largest_scc': len(df_self_loops[df_self_loops['in_largest_scc']]),\n",
    "\n",
    "        'n_fire_cells_in_chronnet_pruned': len(df_fire['cell'].unique()),\n",
    "        'n_fire_cells_in_largest_scc': len(df_fire[df_fire['cell'].isin(largest_scc)]['cell'].unique()),\n",
    "    })\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f7a39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the chronnet_graph_list and chronnet_result_list to pickle files\n",
    "with open(f'chronnet_graph_list.pkl', 'wb') as f:\n",
    "    pickle.dump(chronnet_graph_list, f)\n",
    "with open(f'chronnet_result_list.pkl', 'wb') as f:\n",
    "    pickle.dump(chronnet_result_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca20ffd",
   "metadata": {},
   "source": [
    "# Gird Size Sensitivety Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dc9c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a subset of chronnet_result_list with only some columns\n",
    "\n",
    "\n",
    "\n",
    "keys = [\n",
    "    'grid_size',\n",
    "    'd_max',\n",
    "    'total_nodes_before_pruning',\n",
    "    'total_edges_before_pruning',\n",
    "    'total_strength_before_pruning',\n",
    "    'total_nodes_after_pruning',\n",
    "    'total_edges_after_pruning',\n",
    "    'total_strength_after_pruning',\n",
    "    'largest_scc_nodes',\n",
    "    'largest_scc_ratio',\n",
    "    'n_all_fire-events',\n",
    "    'n_network_events',\n",
    "    'n_sf_events',\n",
    "    'n_self_loops_events',\n",
    "    'n_network_fire',\n",
    "    'n_sf_fire',\n",
    "    'n_self_loops',\n",
    "    'n_network_fire_in_largest_scc',\n",
    "    'n_sf_fire_in_largest_scc',\n",
    "    'n_self_loops_in_largest_scc'\n",
    "]\n",
    "df = pd.DataFrame(chronnet_result_list)[keys]\n",
    "df['ratio_net_events'] = df['n_network_events'] / df['n_all_fire-events']\n",
    "df['ratio_sf_events'] = df['n_sf_events'] / df['n_all_fire-events']\n",
    "df['ratio_self_loops_events'] = df['n_self_loops_events'] / df['n_all_fire-events']\n",
    "\n",
    "\n",
    "df['ratio_net']= df['n_network_fire'] /(df['n_network_fire'] + df['n_sf_fire'] + df['n_self_loops'])\n",
    "df['ratio_sf']= df['n_sf_fire'] /(df['n_network_fire'] + df['n_sf_fire'] + df['n_self_loops'])\n",
    "df['ratio_self_loops']= df['n_self_loops'] /(df['n_network_fire'] + df['n_sf_fire'] + df['n_self_loops'])\n",
    "\n",
    "\n",
    "df['ratio_net_in_largest_scc']= df['n_network_fire_in_largest_scc'] /(df['n_network_fire_in_largest_scc'] + df['n_sf_fire_in_largest_scc'] + df['n_self_loops_in_largest_scc'])\n",
    "df['ratio_sf_in_largest_scc']= df['n_sf_fire_in_largest_scc'] /(df['n_network_fire_in_largest_scc'] + df['n_sf_fire_in_largest_scc'] + df['n_self_loops_in_largest_scc'])\n",
    "df['ratio_self_loops_in_largest_scc']= df['n_self_loops_in_largest_scc'] /(df['n_network_fire_in_largest_scc'] + df['n_sf_fire_in_largest_scc'] + df['n_self_loops_in_largest_scc'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc1bf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adjustText import adjust_text\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "min_size, max_size = 100, 200\n",
    "nn = df['largest_scc_nodes']\n",
    "nn_min, nn_max = nn.min(), nn.max()\n",
    "df['size'] = (nn - nn_min) / (nn_max - nn_min) * (max_size - min_size) + min_size\n",
    "\n",
    "unique_sizes = np.sort(df['grid_size'].unique())\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(unique_sizes)))\n",
    "color_map = dict(zip(unique_sizes, colors))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "texts = []\n",
    "for gs, sub in df.groupby('grid_size'):\n",
    "    ax.scatter(\n",
    "        sub['largest_scc_ratio'],\n",
    "        sub['ratio_net_in_largest_scc'] + sub['ratio_self_loops_in_largest_scc'],\n",
    "        color=color_map[gs],\n",
    "        s=sub['size'],\n",
    "        alpha=0.7,\n",
    "        label=f'{gs}m'  \n",
    "    )\n",
    "    for x, y in zip(\n",
    "        sub['largest_scc_ratio'],\n",
    "        sub['ratio_net_in_largest_scc'] + sub['ratio_self_loops_in_largest_scc']\n",
    "    ):\n",
    "        texts.append(ax.text(x, y, f'{gs}m', fontsize=18))\n",
    "\n",
    "adjust_text(\n",
    "    texts,\n",
    "    arrowprops=dict(arrowstyle='-', color='black', lw=0.5),\n",
    "    expand_text=(1.1, 1.1),\n",
    "    expand_points=(1.2, 1.2)\n",
    ")\n",
    "\n",
    "\n",
    "ax.set_xlabel('largest SCC node %', fontsize=18)\n",
    "ax.set_ylabel('network events ratio in largest SCC', fontsize=18)\n",
    "\n",
    "\n",
    "ax.tick_params(axis='both', labelsize=18)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcffae9",
   "metadata": {},
   "source": [
    "# Network Topology Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aab12f",
   "metadata": {},
   "source": [
    "## Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2943eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grid_sizes = [2000, 3000, 5000]\n",
    "\n",
    "network_list = {\n",
    "    entry['grid_size']: entry['chronnet_pruned']\n",
    "    for entry in chronnet_graph_list\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(nrows=len(grid_sizes), ncols=3, figsize=(16, 12))\n",
    "\n",
    "# Plot degree distributions for each grid size and its top 3 SCCs\n",
    "for i, size in enumerate(grid_sizes):\n",
    "    graph = network_list.get(size)\n",
    "    \n",
    "    # Full graph degree distribution\n",
    "    degrees = [d for _, d in graph.degree()]\n",
    "    total_nodes = graph.number_of_nodes()\n",
    "    deg_counts = Counter(degrees)\n",
    "    degs = sorted(deg_counts)\n",
    "    props = [deg_counts[d] / total_nodes for d in degs]\n",
    "    \n",
    "    ax = axes[i, 0]\n",
    "    ax.scatter(degs, props)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_title(f'Grid Size {size} - Full Graph')\n",
    "    ax.set_xlabel('Degree')\n",
    "    ax.set_ylabel('Proportion of Nodes')\n",
    "    \n",
    "    # Top 3 SCCs degree distributions\n",
    "    sccs = sorted(nx.strongly_connected_components(graph), key=len, reverse=True)[:2]\n",
    "    for j, scc in enumerate(sccs, start=1):\n",
    "        subg = graph.subgraph(scc)\n",
    "        degrees_scc = [d for _, d in subg.degree()]\n",
    "        total_scc = subg.number_of_nodes()\n",
    "        counts_scc = Counter(degrees_scc)\n",
    "        deg_vals_scc = sorted(counts_scc)\n",
    "        props_scc = [counts_scc[d] / total_scc for d in deg_vals_scc]\n",
    "        \n",
    "        ax = axes[i, j]\n",
    "        ax.scatter(deg_vals_scc, props_scc)\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_title(f'Grid Size {size} - SCC {j}')\n",
    "        ax.set_xlabel('Degree')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbf2a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "grid_sizes = [2000, 3000, 5000]\n",
    "\n",
    "network_list = {\n",
    "    entry['grid_size']: entry['chronnet_pruned']\n",
    "    for entry in chronnet_graph_list\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(nrows=len(grid_sizes), ncols=3, figsize=(16, 12))\n",
    "\n",
    "# Plot degree distributions for each grid size and its top 3 SCCs\n",
    "for i, size in enumerate(grid_sizes):\n",
    "    graph = network_list.get(size)\n",
    "    \n",
    "    # Full graph degree distribution\n",
    "    degrees = [d for _, d in graph.degree()]\n",
    "    total_nodes = graph.number_of_nodes()\n",
    "    deg_counts = Counter(degrees)\n",
    "    degs = sorted(deg_counts)\n",
    "    props = [deg_counts[d] / total_nodes for d in degs]\n",
    "    \n",
    "    ax = axes[i, 0]\n",
    "    ax.bar(degs, props, width=0.8, color='skyblue', edgecolor='k')\n",
    "   # ax.scatter(degs, props)\n",
    "    ax.set_title(f'Grid Size {size} - Full Graph')\n",
    "    ax.set_xlabel('Degree')\n",
    "    ax.set_ylabel('Proportion of Nodes')\n",
    "    \n",
    "    # Top 3 SCCs degree distributions\n",
    "    sccs = sorted(nx.strongly_connected_components(graph), key=len, reverse=True)[:2]\n",
    "    for j, scc in enumerate(sccs, start=1):\n",
    "        subg = graph.subgraph(scc)\n",
    "        degrees_scc = [d for _, d in subg.degree()]\n",
    "        total_scc = subg.number_of_nodes()\n",
    "        counts_scc = Counter(degrees_scc)\n",
    "        deg_vals_scc = sorted(counts_scc)\n",
    "        props_scc = [counts_scc[d] / total_scc for d in deg_vals_scc]\n",
    "        \n",
    "        ax = axes[i, j]\n",
    "        ax.bar(deg_vals_scc, props_scc, width=0.8, color='skyblue', edgecolor='k')\n",
    "        #ax.scatter(deg_vals_scc, props_scc)\n",
    "        \n",
    "        ax.set_title(f'Grid Size {size} - SCC {j}')\n",
    "        ax.set_xlabel('Degree')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05274ba3",
   "metadata": {},
   "source": [
    "## Strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256ab772",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grid_sizes = [2000, 3000, 5000]\n",
    "\n",
    "network_list = {\n",
    "    entry['grid_size']: entry['chronnet_pruned']\n",
    "    for entry in chronnet_graph_list\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(nrows=len(grid_sizes), ncols=3, figsize=(16, 12))\n",
    "\n",
    "# Plot strength distributions for each grid size and its top 2 SCCs\n",
    "for i, size in enumerate(grid_sizes):\n",
    "    graph = network_list.get(size)\n",
    "    \n",
    "    # Full graph strength distribution\n",
    "    strengths = [s for _, s in graph.degree(weight='weight')]\n",
    "    total_nodes = graph.number_of_nodes()\n",
    "    str_counts = Counter(strengths)\n",
    "    strs = sorted(str_counts)\n",
    "    props = [str_counts[s] / total_nodes for s in strs]\n",
    "    \n",
    "    ax = axes[i, 0]\n",
    "    ax.scatter(strs, props)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_title(f'Grid Size {size} - Full Graph Strength')\n",
    "    ax.set_xlabel('Strength') # (log scale)\n",
    "    ax.set_ylabel('Proportion of Nodes') #(log scale)\n",
    "    \n",
    "    # Top 2 SCCs strength distributions\n",
    "    sccs = sorted(nx.strongly_connected_components(graph), key=len, reverse=True)[:2]\n",
    "    for j, scc in enumerate(sccs, start=1):\n",
    "        subg = graph.subgraph(scc)\n",
    "        strengths_scc = [s for _, s in subg.degree(weight='weight')]\n",
    "        total_scc = subg.number_of_nodes()\n",
    "        counts_scc = Counter(strengths_scc)\n",
    "        str_vals_scc = sorted(counts_scc)\n",
    "        props_scc = [counts_scc[s] / total_scc for s in str_vals_scc]\n",
    "        \n",
    "        ax = axes[i, j]\n",
    "        ax.scatter(str_vals_scc, props_scc)\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_title(f'Grid Size {size} - SCC {j} Strength')\n",
    "        ax.set_xlabel('Strength ') #(log scale)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1110a0d5",
   "metadata": {},
   "source": [
    "## Compute Topology metrics for each SCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963ff11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "grid_sizes = [2000, 3000, 5000]\n",
    "\n",
    "network_list = {\n",
    "    entry['grid_size']: entry['chronnet_pruned']\n",
    "    for entry in chronnet_graph_list\n",
    "}\n",
    "results = []\n",
    "for size in grid_sizes:\n",
    "    graph = network_list.get(size)\n",
    "    N_full = graph.number_of_nodes()\n",
    "    M_full = graph.number_of_edges()\n",
    "    \n",
    "    \n",
    "    sccs = sorted(nx.strongly_connected_components(graph), key=len, reverse=True)[:2]\n",
    "    for j, scc in enumerate(sccs, start=1):\n",
    "        subg = graph.subgraph(scc).copy()\n",
    "        N_sub = subg.number_of_nodes()\n",
    "        M_sub = subg.number_of_edges()\n",
    "        prop_nodes = N_sub / N_full\n",
    "        prop_edges = M_sub / M_full\n",
    "        \n",
    "        if N_sub > 1:\n",
    "            L = nx.average_shortest_path_length(subg)\n",
    "        else:\n",
    "            L = 0.0\n",
    "        \n",
    "        C = nx.average_clustering(subg)\n",
    "\n",
    "        results.append({\n",
    "            'grid_size': size,\n",
    "            'scc_rank': j,\n",
    "            'nodes': N_sub,\n",
    "            'edges': M_sub,\n",
    "            'prop_nodes': prop_nodes,\n",
    "            'prop_edges': prop_edges,\n",
    "            'avg_path_length': L,\n",
    "            'transitivity': C\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "df = df[\n",
    "    ['grid_size', 'scc_rank', 'nodes', 'edges',\n",
    "     'prop_nodes', 'prop_edges',\n",
    "     'avg_path_length', 'transitivity']\n",
    "]\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c63916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "grid_sizes = [2000, 3000, 5000]\n",
    "\n",
    "network_list = {\n",
    "    entry['grid_size']: entry['chronnet_pruned']\n",
    "    for entry in chronnet_graph_list\n",
    "}\n",
    "\n",
    "results = []\n",
    "for size in grid_sizes:\n",
    "    graph = network_list.get(size)\n",
    "    N_full = graph.number_of_nodes()\n",
    "    M_full = graph.number_of_edges()\n",
    "    \n",
    "    sccs = sorted(nx.strongly_connected_components(graph), key=len, reverse=True)[:2]\n",
    "    for j, scc in enumerate(sccs, start=1):\n",
    "        subg = graph.subgraph(scc).copy()\n",
    "        N_sub = subg.number_of_nodes()\n",
    "        M_sub = subg.number_of_edges()\n",
    "        prop_nodes = N_sub / N_full\n",
    "        prop_edges = M_sub / M_full\n",
    "        \n",
    "        # Observed metrics\n",
    "        if N_sub > 1:\n",
    "            L = nx.average_shortest_path_length(subg)\n",
    "        else:\n",
    "            L = 0.0\n",
    "        C = nx.average_clustering(subg)\n",
    "\n",
    "        # Small-world coefficient: compare to null model\n",
    "        L_rands = []\n",
    "        C_rands = []\n",
    "        for _ in range(10):  # 10 random samples\n",
    "            G_rand = nx.gnm_random_graph(n=N_sub, m=M_sub, directed=True)\n",
    "            # compute metrics on largest SCC of random graph\n",
    "            comp = max(nx.strongly_connected_components(G_rand), key=len)\n",
    "            Gc = G_rand.subgraph(comp)\n",
    "            if Gc.number_of_nodes() > 1:\n",
    "                L_rands.append(nx.average_shortest_path_length(Gc))\n",
    "            else:\n",
    "                L_rands.append(np.nan)\n",
    "            C_rands.append(nx.average_clustering(Gc))\n",
    "        L_rand = np.nanmean(L_rands)\n",
    "        C_rand = np.mean(C_rands)\n",
    "        \n",
    "        if L_rand and L:\n",
    "            sigma = (C / C_rand) / (L / L_rand)\n",
    "        else:\n",
    "            sigma = np.nan\n",
    "\n",
    "        results.append({\n",
    "            'grid_size': size,\n",
    "            'scc_rank': j,\n",
    "            'nodes': N_sub,\n",
    "            'edges': M_sub,\n",
    "            'prop_nodes': prop_nodes,\n",
    "            'prop_edges': prop_edges,\n",
    "            'avg_path_length': L,\n",
    "            'transitivity': C,\n",
    "            'small_worldness': sigma\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "# Reorder columns\n",
    "df = df[[\n",
    "    'grid_size', 'scc_rank', 'nodes', 'edges',\n",
    "    'prop_nodes', 'prop_edges',\n",
    "    'avg_path_length', 'transitivity', 'small_worldness'\n",
    "]]\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae99759f",
   "metadata": {},
   "source": [
    "# Compute Ground Truth and node level metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cbcf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "min_weight =2\n",
    "dmax_scale=2\n",
    "grid_sizes=[2000,3000, 5000]\n",
    "dmax_vals = [x * dmax_scale * np.sqrt(3) for x in grid_sizes]\n",
    "time_bin= '12h'  # time bin for grouping fire events\n",
    "\n",
    "shapefile_path = \"/Users/mabelhu/Desktop/Code/DL_FIRE_SV-C2_576237/fire_archive_SV-C2_576237.shp\"\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "gdf = gdf[gdf['CONFIDENCE'].isin(['h', 'n'])]\n",
    "# transform to meter-based\n",
    "if gdf.crs.to_string() == 'EPSG:4326':\n",
    "    gdf = gdf.to_crs(epsg=3857)\n",
    "# convert to datetime\n",
    "gdf['ACQ_TIME'] = gdf['ACQ_TIME'].astype(str).str.zfill(4)\n",
    "gdf['acq_time'] = pd.to_datetime(\n",
    "    gdf['ACQ_DATE'].astype(str) + ' ' + gdf['ACQ_TIME'],\n",
    "    format='%Y-%m-%d %H%M'\n",
    ")\n",
    "gdf['acq_time'] = gdf['acq_time'].apply(pd.Timestamp)\n",
    "\n",
    "\n",
    "all_y_true      = {}\n",
    "all_y_true_sf   = {}\n",
    "all_y_true_net  = {}\n",
    "all_y_true_sis  = {}\n",
    "\n",
    "\n",
    "burning_result_list = []\n",
    "node_metrics_list = []\n",
    "\n",
    "for hex_size, d_max in zip(grid_sizes, dmax_vals):\n",
    "\n",
    "    hex_grid = cu.create_hex_grid(gdf,hex_size)\n",
    "    gdf_sjoined = gpd.sjoin(\n",
    "        gdf,\n",
    "        hex_grid[['cell', 'geometry']],\n",
    "        how='left',\n",
    "        predicate='within',\n",
    "        rsuffix='_hex'\n",
    "    ).dropna(subset=['cell'])\n",
    "\n",
    "    gdf_sjoined['time_group'] = gdf_sjoined['acq_time'].dt.floor(time_bin)\n",
    "\n",
    "    valid_cells = gdf_sjoined['cell'].unique()\n",
    "    hex_grid_filtered = hex_grid[hex_grid['cell'].isin(valid_cells)]\n",
    "\n",
    "    chronnet = cu.build_chronnet_freq(gdf_sjoined, dmax=d_max, freq=time_bin)\n",
    "    subG = cu.prune_chronnet(chronnet, min_weight=min_weight)\n",
    "\n",
    "    print(f\"Grid size: {hex_size}, Number of nodes: {len(subG.nodes())}, Number of edges: {len(subG.edges())}\")\n",
    "    valid_cells = set(subG.nodes())\n",
    "    df_fire = gdf_sjoined[gdf_sjoined['cell'].isin(valid_cells)].copy()\n",
    "\n",
    "\n",
    "    burning_by_time = df_fire.groupby('time_group')['cell'].apply(set).to_dict()\n",
    "    network_events = []\n",
    "    sf_events = []\n",
    "    self_loops_events = []\n",
    "    for t, cells in burning_by_time.items():\n",
    "        prev_t = t - pd.Timedelta(hours=12)\n",
    "        prev_cells = burning_by_time.get(prev_t, set())\n",
    "        for node in cells:\n",
    "            neis = set(subG.neighbors(node))\n",
    "            if prev_cells.intersection(neis):\n",
    "                network_events.append((node, t))\n",
    "            else:\n",
    "                sf_events.append((node, t))\n",
    "\n",
    "\n",
    "    cc_node_list = list(subG.nodes())\n",
    "\n",
    "    \n",
    "\n",
    "    #### Network events\n",
    "    df_network_fire = pd.DataFrame(network_events, columns=['cell', 'time_group']).drop_duplicates()\n",
    "\n",
    "    grouped_net = df_network_fire.groupby('cell').size().reset_index(name='burning_count_net')\n",
    "    count_dict_net = dict(zip(grouped_net['cell'], grouped_net['burning_count_net']))\n",
    "\n",
    "\n",
    "    y_true_network = np.array([count_dict_net.get(node, 0) for node in cc_node_list], dtype=float)\n",
    "\n",
    "    time_cnt = df_fire['time_group'].nunique()\n",
    "    y_true_network = y_true_network/time_cnt\n",
    "\n",
    "\n",
    "    ##### SF events\n",
    "    df_sf_fire = pd.DataFrame(sf_events, columns=['cell', 'time_group']).drop_duplicates()\n",
    "    grouped_sf = df_sf_fire.groupby('cell').size().reset_index(name='burning_count_sf')\n",
    "    count_dict_sf = dict(zip(grouped_sf['cell'], grouped_sf['burning_count_sf']))\n",
    "    y_true_sf = np.array([count_dict_sf.get(node, 0) for node in cc_node_list], dtype=float)\n",
    "\n",
    "    time_cnt = df_fire['time_group'].nunique()\n",
    "    y_true_sf = y_true_sf / time_cnt\n",
    "\n",
    "\n",
    "    ### all fire events\n",
    "\n",
    "    grouped_df_fire_unique = df_fire[['cell','time_group']].drop_duplicates()\n",
    "    grouped_burning_count_per_cell = grouped_df_fire_unique.groupby('cell').size().reset_index(name='burning_count')\n",
    "    count_dict = dict(zip(grouped_burning_count_per_cell['cell'],\n",
    "                          grouped_burning_count_per_cell['burning_count']))\n",
    "    y_true = np.array([count_dict.get(node, 0)\n",
    "                       for node in cc_node_list], dtype=float)\n",
    "    time_cnt = len(df_fire['time_group'].unique() )\n",
    "    y_true = y_true / time_cnt\n",
    "\n",
    "\n",
    "    # y_true_sis\n",
    "    y_true_sis = (y_true - y_true_sf)/(1-y_true_sf) \n",
    "\n",
    "\n",
    "    # dict\n",
    "    y_true_dict = dict(zip(cc_node_list, y_true))\n",
    "    y_true_sf_dict = dict(zip(cc_node_list, y_true_sf))\n",
    "    y_true_network_dict = dict(zip(cc_node_list, y_true_network))\n",
    "    y_true_sis_dict = dict(zip(cc_node_list, y_true_sis))\n",
    "\n",
    "\n",
    "    all_y_true[hex_size]     = y_true_dict\n",
    "    all_y_true_sf[hex_size]  = y_true_sf_dict\n",
    "    all_y_true_net[hex_size] = y_true_network_dict\n",
    "    all_y_true_sis[hex_size] = y_true_sis_dict\n",
    "\n",
    "    burning_result_list.append({\n",
    "        'grid_size': hex_size,\n",
    "        'y_true': y_true_dict,\n",
    "        'y_true_sf': y_true_sf_dict,\n",
    "        'y_true_network': y_true_network_dict,\n",
    "        'y_true_sis': y_true_sis_dict}\n",
    "    )\n",
    "\n",
    "    hex_grid_filtered['network_p'] = hex_grid_filtered['cell'].map(y_true_network_dict).fillna(0)\n",
    "    hex_grid_filtered['sf_p'] = hex_grid_filtered['cell'].map(y_true_sf_dict).fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ######### plot\n",
    "    #fig, ax = plt.subplots(1, 1, figsize=(6, 6), dpi=300)\n",
    "    #hex_grid_filtered.plot(column='network_p', cmap='hot', legend=True, ax=ax,)# edgecolor='gray'\n",
    "    #plt.title(\"Historical Network-driven Fire Probability Distribution\")\n",
    "    #plt.axis('equal')  # Ensure proper aspect ratio\n",
    "    #fig.savefig(f\"network_probability_size_{hex_size}.pdf\", format='pdf', bbox_inches='tight')\n",
    "    #plt.show()\n",
    "#\n",
    "    #fig, ax = plt.subplots(1, 1, figsize=(6, 6), dpi=300)\n",
    "    #hex_grid_filtered.plot(column='sf_p', cmap='hot', legend=True, ax=ax,)# edgecolor='gray'\n",
    "    #plt.title(\"Historical Spontaneous Fire Probability Distribution\")\n",
    "    #plt.axis('equal')  # Ensure proper aspect ratio\n",
    "    #fig.savefig(f\"sf_probability_size_{hex_size}.pdf\", format='pdf', bbox_inches='tight')\n",
    "    #plt.show()\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "    ######### compute node metrics\n",
    "    in_deg_dict = dict(subG.in_degree())\n",
    "    out_deg_dict = dict(subG.out_degree())\n",
    "    in_str_dict = dict(subG.in_degree(weight='weight'))\n",
    "    out_str_dict = dict(subG.out_degree(weight='weight'))\n",
    "\n",
    "\n",
    "    deg_df = pd.DataFrame({\n",
    "    'cell':        cc_node_list,\n",
    "    'degree':     [subG.degree(cell) for cell in cc_node_list],\n",
    "    'strength':   [subG.degree(cell, weight='weight') for cell in cc_node_list],\n",
    "    'in_degree':   [in_deg_dict[cell]   for cell in cc_node_list],\n",
    "    'out_degree':  [out_deg_dict[cell]  for cell in cc_node_list],\n",
    "    'in_strength': [in_str_dict[cell]   for cell in cc_node_list],\n",
    "    'out_strength':[out_str_dict[cell]  for cell in cc_node_list],\n",
    "    'v_true_sis': [y_true_sis_dict[cell] for cell in cc_node_list],\n",
    "    'v_true_sf':  [y_true_sf_dict[cell]  for cell in cc_node_list],\n",
    "    'v_true_net': [y_true_network_dict[cell] for cell in cc_node_list],\n",
    "    'v_true':     [y_true_dict[cell]    for cell in cc_node_list]\n",
    "    })\n",
    "\n",
    "\n",
    "    # merge the degree dataframe with the burning count dataframe\n",
    "    node_metrics = (grouped_burning_count_per_cell.merge(deg_df, on='cell', how='left')\n",
    "                 .merge(grouped_sf, on='cell', how='left')\n",
    "                 .merge(grouped_net, on='cell', how='left')\n",
    "    )\n",
    "    node_metrics_list.append({\n",
    "        'grid_size': hex_size,\n",
    "        'node_metrics': node_metrics\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9c0760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save node_metrics_list and burning_result_list to pickle files\n",
    "with open(f'node_metrics_list.pkl', 'wb') as f:\n",
    "    pickle.dump(node_metrics_list, f)   \n",
    "\n",
    "with open(f'burning_result_list.pkl', 'wb') as f:\n",
    "    pickle.dump(burning_result_list, f) \n",
    "# save all_y_true, all_y_true_sf, all_y_true_net, all_y_true_sis to pickle files\n",
    "with open(f'all_y_true.pkl', 'wb') as f:\n",
    "    pickle.dump(all_y_true, f)\n",
    "with open(f'all_y_true_sf.pkl', 'wb') as f:\n",
    "    pickle.dump(all_y_true_sf, f)       \n",
    "with open(f'all_y_true_net.pkl', 'wb') as f:\n",
    "    pickle.dump(all_y_true_net, f)\n",
    "with open(f'all_y_true_sis.pkl', 'wb') as f:\n",
    "    pickle.dump(all_y_true_sis, f)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163159e5",
   "metadata": {},
   "source": [
    "# Compute the node-level metrics in SCCs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c8e9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('burning_result_list.pkl', 'rb') as f:\n",
    "    burning_result_list = pickle.load(f)\n",
    "\n",
    "all_y_true_sis = {\n",
    "    item['grid_size']: item['y_true_sis']\n",
    "    for item in burning_result_list\n",
    "}\n",
    "all_y_true_sf = {\n",
    "    item['grid_size']: item['y_true_sf']\n",
    "    for item in burning_result_list     \n",
    "}\n",
    "all_y_true_net = {\n",
    "    item['grid_size']: item['y_true_network']       \n",
    "    for item in burning_result_list\n",
    "}\n",
    "all_y_true = {\n",
    "    item['grid_size']: item['y_true']\n",
    "    for item in burning_result_list \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5471f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'chronnet_graph_list.pkl'\n",
    "with open(filename, 'rb') as f:\n",
    "    chronnet_graph_list = pickle.load(f)\n",
    "    \n",
    "grid_sizes = [2000, 3000, 5000]\n",
    "\n",
    "network_list = {\n",
    "    entry['grid_size']: entry['chronnet_pruned']\n",
    "    for entry in chronnet_graph_list\n",
    "}\n",
    "\n",
    "# node_metrics_list \n",
    "with open('node_metrics_list.pkl', 'rb') as f:\n",
    "    node_metrics_list = pickle.load(f)\n",
    "\n",
    "# chronnet_result_list\n",
    "with open('chronnet_result_list.pkl', 'rb') as f:\n",
    "    chronnet_result_list = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f4d985",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_to_gdf = {\n",
    "    entry['grid_size']: entry['gdf_sjoined']\n",
    "    for entry in chronnet_result_list\n",
    "}\n",
    "size_to_gdf[5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f444f438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute eigenvector centrality for the top 2 SCCs in each grid size\n",
    "\n",
    "scc_node_result = []\n",
    "grid_sizes = [2000, 3000, 5000]\n",
    "\n",
    "for grid_size in grid_sizes:\n",
    "\n",
    "    G = network_list.get(grid_size)\n",
    "    sccs = list(nx.strongly_connected_components(G))\n",
    "    top2_sccs = sorted(sccs, key=len, reverse=True)[:2]\n",
    "\n",
    "    y_true_sis = all_y_true_sis.get(grid_size, {})\n",
    "    y_true_sf = all_y_true_sf.get(grid_size, {})\n",
    "    y_true_network = all_y_true_net.get(grid_size, {})      \n",
    "    y_true = all_y_true.get(grid_size, {})  \n",
    "\n",
    "    size_to_gdf = {\n",
    "    entry['grid_size']: entry['gdf_sjoined']\n",
    "    for entry in chronnet_result_list\n",
    "    }\n",
    "    \n",
    "    gdf_sjoined = size_to_gdf.get(grid_size, pd.DataFrame())\n",
    "\n",
    "    for rank, scc_nodes in enumerate(top2_sccs, start=1):\n",
    "        subG = G.subgraph(scc_nodes).copy()\n",
    "        cc_node_list = list(subG.nodes())\n",
    "\n",
    "        # Compute node metrics\n",
    "        in_deg_dict = dict(subG.in_degree())\n",
    "        out_deg_dict = dict(subG.out_degree())\n",
    "        in_str_dict = dict(subG.in_degree(weight='weight'))\n",
    "        out_str_dict = dict(subG.out_degree(weight='weight'))\n",
    "        deg_dict = dict(subG.degree())\n",
    "        str_dict = dict(subG.degree(weight='weight'))\n",
    "\n",
    "        ### !!!!! scc\n",
    "        scc_y_true_list= compute_y_true(\n",
    "            gdf_sjoined,\n",
    "            subG,\n",
    "            cell_col='cell',\n",
    "            time_col='time_group',\n",
    "            lag_hours=12\n",
    "        )\n",
    "        scc_y_true_dict=scc_y_true_list['y_true_dict']\n",
    "        scc_y_true_sf_dict=scc_y_true_list['y_true_sf_dict']\n",
    "        scc_y_true_network_dict=scc_y_true_list['y_true_network_dict']\n",
    "        scc_y_true_sis_dict=scc_y_true_list['y_true_sis_dict']\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "        subG_undir = subG.to_undirected() # ! only one edge is created with an arbitrary choice of which edge data to use.\n",
    "        ev_dict = nx.eigenvector_centrality(\n",
    "            subG,\n",
    "            max_iter=1000,\n",
    "            tol=1e-6\n",
    "            #,\n",
    "            #weight='weight'\n",
    "        )\n",
    "        ev_dict_w=nx.eigenvector_centrality(\n",
    "            subG,\n",
    "            max_iter=1000,\n",
    "            tol=1e-6,\n",
    "            weight='weight'\n",
    "        )\n",
    "        betweeness_dict = nx.betweenness_centrality(\n",
    "            subG,\n",
    "            normalized=True\n",
    "          #  weight='weight'\n",
    "        )\n",
    "        betweeness_dict_w = nx.betweenness_centrality(\n",
    "            subG,\n",
    "            normalized=True,\n",
    "            weight='weight'\n",
    "        )\n",
    "        closeness_dict = nx.closeness_centrality(\n",
    "            subG\n",
    "          #  distance='weight'\n",
    "        )\n",
    "        closeness_dict_w = nx.closeness_centrality(\n",
    "            subG,\n",
    "            distance='weight'\n",
    "        )\n",
    "        clustering_coeffs = nx.clustering(subG_undir)\n",
    "        clastering_coeffs_w = nx.clustering(subG_undir, weight='weight')   \n",
    "\n",
    "        # NetworkX's PageRank already uses a row‑stochastic transition matrix (out‑degree normalised).\n",
    "        pagerank_dict = nx.pagerank(subG, alpha=0.85, max_iter=100, tol=1e-6,weight=None)\n",
    "        pagerank_dict_w = nx.pagerank(subG, alpha=0.85, max_iter=100, tol=1e-6, weight='weight') \n",
    "\n",
    "        scc_node_metrics = pd.DataFrame({\n",
    "            'cell': cc_node_list,\n",
    "            'degree': [deg_dict[cell] for cell in cc_node_list],\n",
    "            'strength': [str_dict[cell] for cell in cc_node_list],\n",
    "            'in_degree': [in_deg_dict[cell] for cell in cc_node_list],\n",
    "            'out_degree': [out_deg_dict[cell] for cell in cc_node_list],\n",
    "            'in_strength': [in_str_dict[cell] for cell in cc_node_list],\n",
    "            'out_strength': [out_str_dict[cell] for cell in cc_node_list],\n",
    "            'clustering_coefficient': [clustering_coeffs[cell] for cell in cc_node_list],\n",
    "            'eigenvector_centrality': [ev_dict[cell] for cell in cc_node_list],\n",
    "            'betweenness_centrality': [betweeness_dict[cell] for cell in cc_node_list],\n",
    "            'closeness_centrality': [closeness_dict[cell] for cell in cc_node_list],\n",
    "            'eigenvector_centrality_w': [ev_dict_w[cell] for cell in cc_node_list],\n",
    "            'betweenness_centrality_w': [betweeness_dict_w[cell] for cell in cc_node_list],\n",
    "            'closeness_centrality_w': [closeness_dict_w[cell] for cell in cc_node_list],\n",
    "            'clustering_coefficient_w': [clastering_coeffs_w.get(cell, 0) for cell in cc_node_list],\n",
    "            'pagerank': [pagerank_dict[c] for c in cc_node_list],\n",
    "            'pagerank_w': [pagerank_dict_w[c] for c in cc_node_list],\n",
    "            \n",
    "            'v_true_sis': [y_true_sis.get(cell, 0) for cell in cc_node_list],\n",
    "            'v_true_net': [y_true_network.get(cell, 0) for cell in cc_node_list],\n",
    "            'v_true_sf': [y_true_sf.get(cell, 0) for cell in cc_node_list],\n",
    "            'v_true': [y_true.get(cell, 0) for cell in cc_node_list],\n",
    "\n",
    "            'v_true_scc': [scc_y_true_dict.get(cell, 0) for cell in cc_node_list],\n",
    "            'v_true_scc_net': [scc_y_true_network_dict.get(cell, 0) for cell in cc_node_list],\n",
    "            'v_true_scc_sf': [scc_y_true_sf_dict.get(cell, 0) for cell in cc_node_list],    \n",
    "            'v_true_scc_sis': [scc_y_true_sis_dict.get(cell, 0) for cell in cc_node_list]\n",
    "\n",
    "        })\n",
    "        \n",
    "        scc_node_result.append({\n",
    "            'grid_size': grid_size,\n",
    "            'scc_rank': rank,\n",
    "            'node_metrics': scc_node_metrics\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f82986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the scc_node_result to pickle file\n",
    "with open('scc_node_result.pkl', 'wb') as f:\n",
    "    pickle.dump(scc_node_result, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b72da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the results\n",
    "\n",
    "for rec in scc_node_result:\n",
    "    print(f\"grid_size={rec['grid_size']}, scc_rank={rec['scc_rank']}, rows={len(rec['node_metrics'])}\")\n",
    "    display(rec['node_metrics'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859e31b2",
   "metadata": {},
   "source": [
    "## scc_eval_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44311195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the scc_node_result.pkl file\n",
    "import pickle\n",
    "with open('scc_node_result.pkl', 'rb') as f:\n",
    "    scc_node_result = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecefc5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute JSD , recognition quality between node metrics and v_true_sis \n",
    "\n",
    "scc_eval_list = []\n",
    "\n",
    "\n",
    "for rec in scc_node_result:\n",
    "    node_metrics = rec['node_metrics']\n",
    "    v_true_sis = node_metrics['v_true_sis'].values\n",
    "    for metric in ['degree','strength','eigenvector_centrality','eigenvector_centrality_w', 'betweenness_centrality', 'closeness_centrality','pagerank', 'pagerank_w']:\n",
    "        v_metric = node_metrics[metric].values\n",
    "        jsd_val = jsd_from_samples(v_metric, v_true_sis, scale='minmax')\n",
    "        rq_val = recognition_quality(v_metric, v_true_sis)\n",
    "\n",
    "\n",
    "        rho = spearmanr(v_metric, v_true_sis).correlation\n",
    "        ken = kendalltau(v_metric, v_true_sis).correlation\n",
    "        ndcg_100 = ndcg_score([v_metric], [v_true_sis], k=100)\n",
    "        ndcg_300 = ndcg_score([v_metric], [v_true_sis], k=300)\n",
    "        ndcg_500 = ndcg_score([v_metric], [v_true_sis], k=500)\n",
    "        ndcg_1000= ndcg_score([v_metric], [v_true_sis], k=1000)\n",
    "        k1 = int(len(v_metric) * 0.01)\n",
    "        k5 = int(len(v_metric) * 0.05)\n",
    "        k10 = int(len(v_metric) * 0.1)\n",
    "        k20 = int(len(v_metric) * 0.2)\n",
    "        k50 = int(len(v_metric) * 0.5)  \n",
    "        nd_1p = ndcg_score([v_metric], [v_true_sis], k=k1)\n",
    "        nd_5p = ndcg_score([v_metric], [v_true_sis], k=k5)\n",
    "        nd_10p = ndcg_score([v_metric], [v_true_sis], k=k10)\n",
    "        nd_20p = ndcg_score([v_metric], [v_true_sis], k=k20)        \n",
    "        nd_50p = ndcg_score([v_metric], [v_true_sis], k=k50)\n",
    "\n",
    "\n",
    "        scc_eval_list.append({\n",
    "            'grid_size': rec['grid_size'],\n",
    "            'scc_rank': rec['scc_rank'],\n",
    "            'metric': metric,\n",
    "            'JSD': jsd_val,\n",
    "            'RQ': rq_val,\n",
    "            'rho': rho,\n",
    "            'ken': ken,\n",
    "            'ndcg_100': ndcg_100,\n",
    "            'ndcg_300': ndcg_300,\n",
    "            'ndcg_500': ndcg_500,\n",
    "            'ndcg_1000': ndcg_1000,\n",
    "            'ndcg_1p': nd_1p,\n",
    "            'ndcg_5p': nd_5p,\n",
    "            'ndcg_10p': nd_10p,\n",
    "            'ndcg_20p': nd_20p,\n",
    "            'ndcg_50p': nd_50p\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "scc_eval_df = pd.DataFrame(scc_eval_list)\n",
    "scc_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883c605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the scc_eval_df to csv file\n",
    "\n",
    "scc_eval_df.to_csv('scc_eval_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b0a50d",
   "metadata": {},
   "source": [
    "# SIS modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09686688",
   "metadata": {},
   "source": [
    "## Sweep Tau experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a520163",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('burning_result_list.pkl', 'rb') as f:\n",
    "    burning_result_list = pickle.load(f)\n",
    "\n",
    "all_y_true_sis = {\n",
    "    item['grid_size']: item['y_true_sis']\n",
    "    for item in burning_result_list\n",
    "}\n",
    "all_y_true_sf = {\n",
    "    item['grid_size']: item['y_true_sf']\n",
    "    for item in burning_result_list     \n",
    "}\n",
    "all_y_true_net = {\n",
    "    item['grid_size']: item['y_true_network']       \n",
    "    for item in burning_result_list\n",
    "}\n",
    "all_y_true = {\n",
    "    item['grid_size']: item['y_true']\n",
    "    for item in burning_result_list \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f04e4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "grid_sizes = [2000, 3000, 5000]\n",
    "\n",
    "\n",
    "network_list = {\n",
    "    entry['grid_size']: entry['chronnet_pruned']\n",
    "    for entry in chronnet_graph_list\n",
    "}\n",
    "\n",
    "gamma = 1.0\n",
    "tau_c = 1.0\n",
    "tau_vals = np.linspace(1.1 * tau_c, 100 * tau_c, num=100)\n",
    "\n",
    "\n",
    "sis_results_list = []\n",
    "\n",
    "for grid_size in grid_sizes:\n",
    "    G = network_list.get(grid_size)\n",
    "\n",
    "    sccs = list(nx.strongly_connected_components(G))\n",
    "    top2_sccs = sorted(sccs, key=len, reverse=True)[:2]\n",
    "\n",
    "    \n",
    "    for rank, scc_nodes in enumerate(top2_sccs, start=1):\n",
    "        subG = G.subgraph(scc_nodes).copy()\n",
    "        cc_node_list = list(subG.nodes())\n",
    "        W = nx.to_numpy_array(subG, nodelist=cc_node_list, weight='weight', dtype=float)\n",
    "        lambda_max = np.max(np.abs(np.linalg.eigvals(W)))\n",
    "        W_norm = W / lambda_max\n",
    "\n",
    "        for tau in tau_vals:\n",
    "            beta = tau * gamma\n",
    "            print(f\"[grid {grid_size} | SCC {rank}] tau={tau:.3f}, beta={beta:.3f}\")\n",
    "\n",
    "            v_sis = nimfa_sis_steady_state_root_1(W_norm, beta, gamma)\n",
    "            if np.all(v_sis == 0):\n",
    "                print(f\"[grid {grid_size} | SCC {rank}] tau={tau:.3f} not converged, skip\")\n",
    "                continue\n",
    "            \n",
    "\n",
    "\n",
    "            y_true = np.array([all_y_true[grid_size].get(node, 0) for node in cc_node_list], dtype=float)\n",
    "            y_true_sf = np.array([all_y_true_sf[grid_size].get(node, 0) for node in cc_node_list], dtype=float)\n",
    "            y_true_sis = np.array([all_y_true_sis[grid_size].get(node, 0) for node in cc_node_list], dtype=float)\n",
    "            y_true_network = np.array([all_y_true_net[grid_size].get(node, 0) for node in cc_node_list], dtype=float)\n",
    "            v_hybrid = y_true_sf + (1 - y_true_sf) * v_sis\n",
    "            \n",
    "            \n",
    "            # hybrid\n",
    "            rq_val         = recognition_quality(v_hybrid, y_true)\n",
    "            jsd_val        = jsd_from_samples(v_hybrid, y_true, scale='minmax')\n",
    "            mse            = np.mean((v_hybrid - y_true) ** 2)\n",
    "            rmse           = np.sqrt(mse)\n",
    "\n",
    "            # net # jsd_val_net    = jsd_from_samples(v_sis, y_true_sis, scale='none')\n",
    "            rq_val_net     = recognition_quality(v_sis, y_true_sis)\n",
    "            mse_net        = np.mean((v_sis - y_true_sis) ** 2)\n",
    "            rmse_net       = np.sqrt(mse_net)\n",
    "            jsd_val_net_nm = jsd_from_samples(v_sis, y_true_sis, scale='minmax')\n",
    "            \n",
    "            # Compute Kendall and Spearman correlations\n",
    "            \n",
    "            rho=spearmanr(v_sis, y_true_sis).statistic\n",
    "            p_value=spearmanr(v_sis, y_true_sis).pvalue\n",
    "    \n",
    "            nd_50= ndcg_score([y_true_sis], [v_sis], k=50)\n",
    "            nd_100 = ndcg_score([y_true_sis], [v_sis], k=100)\n",
    "            nd_300 = ndcg_score([y_true_sis], [v_sis], k=300)\n",
    "            nd_500 = ndcg_score([y_true_sis], [v_sis], k=500)\n",
    "            nd_1000 = ndcg_score([y_true_sis], [v_sis], k=1000)\n",
    "\n",
    "\n",
    "            # compute the topk% ndcg\n",
    "            k1= int(len(v_sis) * 0.01)\n",
    "            k5= int(len(v_sis) * 0.05)\n",
    "            k10= int(len(v_sis) * 0.1)\n",
    "            k20= int(len(v_sis) * 0.2)\n",
    "            k50= int(len(v_sis) * 0.5)\n",
    "\n",
    "            nd_1p = ndcg_score([y_true_sis], [v_sis], k=k1)\n",
    "            nd_5p = ndcg_score([y_true_sis], [v_sis], k=k5)\n",
    "            nd_10p = ndcg_score([y_true_sis], [v_sis], k=k10)\n",
    "            nd_20p = ndcg_score([y_true_sis], [v_sis], k=k20)\n",
    "            nd_50p = ndcg_score([y_true_sis], [v_sis], k=k50)\n",
    "\n",
    "            \n",
    "            sis_results_list.append({\n",
    "                'grid_size': grid_size,\n",
    "                'scc_rank':      rank,\n",
    "                'tau':           tau,\n",
    "                'beta':          beta,\n",
    "                'gamma':         gamma,\n",
    "                'cc_node_list': cc_node_list,\n",
    "\n",
    "                'RQ':            rq_val,\n",
    "                'JSD':           jsd_val,\n",
    "                'MSE':           mse,\n",
    "                'RMSE':          rmse,\n",
    "                'RQ_net':        rq_val_net,\n",
    "                'MSE_net':       mse_net,\n",
    "                'RMSE_net':      rmse_net,\n",
    "                'JSD_net_norm':  jsd_val_net_nm,\n",
    "\n",
    "                'rho':           rho,\n",
    "                'p_value':       p_value,\n",
    "                'ndcg_50':       nd_50,\n",
    "                'ndcg_100':      nd_100,\n",
    "                'ndcg_300':      nd_300,\n",
    "                'ndcg_500':      nd_500,\n",
    "                'ndcg_1000':     nd_1000,\n",
    "                'ndcg_1p':       nd_1p,\n",
    "                'ndcg_5p':       nd_5p,\n",
    "                'ndcg_10p':      nd_10p,\n",
    "                'ndcg_20p':      nd_20p,\n",
    "                'ndcg_50p':      nd_50p,\n",
    "\n",
    "                'v_sis':         v_sis,\n",
    "                'v_hybrid':      v_hybrid,\n",
    "\n",
    "                'y_true':     y_true,\n",
    "                'y_true_sis': y_true_sis,\n",
    "                'y_true_sf':  y_true_sf,\n",
    "                'y_true_net': y_true_network\n",
    "               \n",
    "            })\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf0952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the sis_results_list to pickle file\n",
    "with open('sis_results_list.pkl', 'wb') as f:\n",
    "    pickle.dump(sis_results_list, f)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67fc0e3",
   "metadata": {},
   "source": [
    "## Select the optimal tau: best_tau_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96ee1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " grid_size  scc_rank   tau_opt   RQ_net  JSD_net_norm\n",
      "      2000         1 16.084848 0.673588      0.624126\n",
      "      2000         2 25.075758 0.754363      0.531153\n",
      "      3000         1 48.052525 0.744169      0.382687\n",
      "      3000         2 17.083838 0.773537      0.156139\n",
      "      5000         1 29.071717 0.761696      0.217230\n",
      "      5000         2 23.077778 0.818132      0.178870\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "FILE = \"sis_results_tau.csv\"          # ← adjust if the file lives elsewhere\n",
    "\n",
    "RQ_COL  = \"RQ_net\"\n",
    "JSD_COL = \"JSD_net_norm\"\n",
    "TAU_COL = \"tau\"\n",
    "\n",
    "def pick_tau_turning_point(df,\n",
    "                           rq_plateau_thresh=2e-3,   # |ΔRQ| below this ⇒ flat\n",
    "                           plateau_window=2,         # how many flat steps in a row\n",
    "                           rq_plateau_frac=0.95):    # fallback: ≥95 % of max RQ\n",
    "    \"\"\"\n",
    "    Given a (grid_size, scc_rank) sub‑DataFrame, return one row containing\n",
    "    tau_opt, RQ_net, JSD_net_norm that best trades off high‑and‑flat RQ vs\n",
    "    still‑low JSD.\n",
    "    \"\"\"\n",
    "    # 1. average repeated runs at the same τ\n",
    "    agg = (df.groupby(TAU_COL)\n",
    "             .agg({RQ_COL: \"mean\", JSD_COL: \"mean\"})\n",
    "             .sort_index()\n",
    "             .reset_index())\n",
    "\n",
    "    # If we only have a couple of τ values, just grab the “best” one.\n",
    "    if len(agg) < 3:\n",
    "        return agg.sort_values([RQ_COL, JSD_COL], ascending=[False, True]).iloc[0]\n",
    "\n",
    "    # 2. finite differences\n",
    "    agg[\"dJSD\"] = agg[JSD_COL].diff()\n",
    "    agg[\"dRQ\"]  = agg[RQ_COL].diff()\n",
    "\n",
    "    best = None\n",
    "    # 3. turning point: JSD just turned upward and RQ has been flat\n",
    "    for i in range(plateau_window, len(agg)):\n",
    "        jsd_turn = agg.loc[i - 1, \"dJSD\"] <= 0 and agg.loc[i, \"dJSD\"] > 0\n",
    "        rq_flat  = (agg.loc[i - plateau_window + 1 : i, \"dRQ\"]\n",
    "                      .abs()\n",
    "                      .lt(rq_plateau_thresh)\n",
    "                      .all())\n",
    "        if jsd_turn and rq_flat:\n",
    "            best = agg.loc[i, [TAU_COL, RQ_COL, JSD_COL]]\n",
    "            break\n",
    "            \n",
    "        if best is None:\n",
    "            max_rq = agg[RQ_COL].max()\n",
    "            plateau = agg[agg[RQ_COL] >= rq_plateau_frac * max_rq]\n",
    "            best = plateau.nsmallest(1, JSD_COL).iloc[0]\n",
    "\n",
    "    better = agg[\n",
    "        (agg[JSD_COL] < best[JSD_COL]) &\n",
    "        (agg[RQ_COL] > best[RQ_COL])\n",
    "    ]\n",
    "    if not better.empty:\n",
    "        best = better.sort_values(RQ_COL, ascending=False).iloc[0]\n",
    "\n",
    "    return best[[TAU_COL, RQ_COL, JSD_COL]]\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------ main driver\n",
    "# df = pd.read_csv(FILE)\n",
    "# df=csv(sis_results_list)\n",
    "df =pd.DataFrame(sis_results_list)\n",
    "\n",
    "\n",
    "best_rows = []\n",
    "for (gsize, rank), sub in df.groupby([\"grid_size\", \"scc_rank\"]):\n",
    "    best = pick_tau_turning_point(sub)\n",
    "    best_rows.append({\n",
    "        \"grid_size\":      gsize,\n",
    "        \"scc_rank\":       rank,\n",
    "        \"tau_opt\":        best[TAU_COL],\n",
    "        RQ_COL:           best[RQ_COL],\n",
    "        JSD_COL:          best[JSD_COL]\n",
    "    })\n",
    "\n",
    "results = pd.DataFrame(best_rows).sort_values([\"grid_size\", \"scc_rank\"])\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "\n",
    "best_tau_df=results\n",
    "# If you need it on disk:\n",
    "results.to_csv(\"best_tau_by_grid_scc.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c859070f",
   "metadata": {},
   "source": [
    "## Scatter plot with baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aba82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "best_tau_df = pd.read_csv('best_tau_by_grid_scc.csv')  \n",
    "scc_eval_df=pd.read_csv('scc_eval_results.csv')\n",
    "sis_results_df = pd.DataFrame(sis_results_list)\n",
    "\n",
    "degree_df = scc_eval_df[scc_eval_df['metric'] == 'pagerank_w']  # contains scc_eval_df[scc_eval_df['metric']=='degree'] results\n",
    "\n",
    "# Create a single 3x2 figure instead of one-per-grid_size\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=3, ncols=2,\n",
    "    figsize=(12, 18),\n",
    "    gridspec_kw={'wspace': 0.3, 'hspace': 0.4, 'right': 0.85}\n",
    ")\n",
    "\n",
    "# Loop over grid_size (rows) and scc_rank (cols)\n",
    "for i, (grid_size, sub_df) in enumerate(sis_results_df.groupby('grid_size')):\n",
    "    for j, (scc_rank, group) in enumerate(sub_df.groupby('scc_rank')):\n",
    "        ax = axes[i, j]\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        scatter = ax.scatter(\n",
    "          \n",
    "            group['JSD_net_norm'],\n",
    "            group['RQ_net'],\n",
    "           \n",
    "            c=group['tau'],\n",
    "            cmap='viridis',\n",
    "            s=30,\n",
    "            alpha=0.7,\n",
    "            label='SIS steady-state infection rate' \n",
    "        )\n",
    "\n",
    "        # --- plot degree baselines ---\n",
    "        # fetch matching row in degree_df\n",
    "        baseline = degree_df[\n",
    "            (degree_df['grid_size'] == grid_size) &\n",
    "            (degree_df['scc_rank']  == scc_rank)\n",
    "        ].iloc[0]\n",
    "\n",
    "        # plot the optiimal tau point\n",
    "        best_row = best_tau_df[(best_tau_df['grid_size'] == grid_size) & (best_tau_df['scc_rank']  == scc_rank)]\n",
    "\n",
    "        if not best_row.empty:\n",
    "            x = best_row['JSD_net_norm'].values[0]\n",
    "            y = best_row['RQ_net'].values[0]\n",
    "            tau_label = f\"τ={best_row['tau_opt'].values[0]:.2f}\"\n",
    "        \n",
    "            ax.scatter(x, y, color='red', s=80, marker='*', label='Optimal τ')\n",
    "            ax.annotate(tau_label, (x, y),\n",
    "                        textcoords=\"offset points\", xytext=(5, -10),\n",
    "                ha='left', fontsize=9, color='red') \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # !! need to change with the scc_eval_df\n",
    "        ax.axvline(baseline['JSD'], linestyle='--', label='Node PageRank baseline')\n",
    "        # horizontal line at degree RQ\n",
    "        ax.axhline(baseline['RQ'],  linestyle='--')\n",
    "\n",
    "        ax.set_xlabel('JSD')\n",
    "        ax.set_ylabel('Average Recall (AR)')\n",
    "        ax.set_title(f'grid_size={grid_size}, scc_rank={scc_rank}')\n",
    "        ax.margins(x=0.05, y=0.05)\n",
    "        ax.legend(loc='lower left', bbox_to_anchor=(0.02, 0.02))\n",
    "        \n",
    "\n",
    "    fig.colorbar(scatter,ax=axes[i, :],             \n",
    "            label='τ value',          \n",
    "            orientation='vertical',\n",
    "            fraction=0.025,           \n",
    "            pad=0.02   )              \n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig.savefig('sis_results_tau.pdf', format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5b13d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_by_size = {\n",
    "    entry['grid_size']: entry['node_metrics']\n",
    "    for entry in node_metrics_list\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e879daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the df to csv \n",
    "sis_df = pd.DataFrame(sis_results_list)\n",
    "sis_df.to_csv(f'sis_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27c7c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(sis_results_list)\n",
    "result_df = df[df['grid_size'] == 5000]\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3436f106",
   "metadata": {},
   "source": [
    "## NDCG : SIS with(the optiaml tau) vs PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4a893f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grid_size</th>\n",
       "      <th>scc_rank</th>\n",
       "      <th>tau</th>\n",
       "      <th>beta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>RQ</th>\n",
       "      <th>JSD</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>RQ_net</th>\n",
       "      <th>...</th>\n",
       "      <th>ndcg_100</th>\n",
       "      <th>ndcg_300</th>\n",
       "      <th>ndcg_500</th>\n",
       "      <th>ndcg_1000</th>\n",
       "      <th>ndcg_1p</th>\n",
       "      <th>ndcg_5p</th>\n",
       "      <th>ndcg_10p</th>\n",
       "      <th>ndcg_20p</th>\n",
       "      <th>ndcg_50p</th>\n",
       "      <th>tau_opt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.084848</td>\n",
       "      <td>16.084848</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.447073</td>\n",
       "      <td>0.433602</td>\n",
       "      <td>0.212329</td>\n",
       "      <td>0.460792</td>\n",
       "      <td>0.673588</td>\n",
       "      <td>...</td>\n",
       "      <td>0.518537</td>\n",
       "      <td>0.656073</td>\n",
       "      <td>0.726481</td>\n",
       "      <td>0.801077</td>\n",
       "      <td>0.398057</td>\n",
       "      <td>0.550256</td>\n",
       "      <td>0.636761</td>\n",
       "      <td>0.735868</td>\n",
       "      <td>0.827745</td>\n",
       "      <td>16.084848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25.075758</td>\n",
       "      <td>25.075758</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.498715</td>\n",
       "      <td>0.464845</td>\n",
       "      <td>0.223227</td>\n",
       "      <td>0.472469</td>\n",
       "      <td>0.754363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.807354</td>\n",
       "      <td>0.843784</td>\n",
       "      <td>0.883469</td>\n",
       "      <td>0.917787</td>\n",
       "      <td>0.667798</td>\n",
       "      <td>0.780181</td>\n",
       "      <td>0.824938</td>\n",
       "      <td>0.840175</td>\n",
       "      <td>0.894838</td>\n",
       "      <td>25.075758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48.052525</td>\n",
       "      <td>48.052525</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.531685</td>\n",
       "      <td>0.324928</td>\n",
       "      <td>0.276043</td>\n",
       "      <td>0.525398</td>\n",
       "      <td>0.744169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570478</td>\n",
       "      <td>0.678075</td>\n",
       "      <td>0.730133</td>\n",
       "      <td>0.794098</td>\n",
       "      <td>0.537202</td>\n",
       "      <td>0.669103</td>\n",
       "      <td>0.742679</td>\n",
       "      <td>0.806082</td>\n",
       "      <td>0.882797</td>\n",
       "      <td>48.052525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.083838</td>\n",
       "      <td>17.083838</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.660871</td>\n",
       "      <td>0.312646</td>\n",
       "      <td>0.046128</td>\n",
       "      <td>0.214775</td>\n",
       "      <td>0.773537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.742701</td>\n",
       "      <td>0.773690</td>\n",
       "      <td>0.787421</td>\n",
       "      <td>0.809069</td>\n",
       "      <td>0.603978</td>\n",
       "      <td>0.734376</td>\n",
       "      <td>0.753121</td>\n",
       "      <td>0.775593</td>\n",
       "      <td>0.804727</td>\n",
       "      <td>17.083838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.071717</td>\n",
       "      <td>29.071717</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.608960</td>\n",
       "      <td>0.407102</td>\n",
       "      <td>0.161796</td>\n",
       "      <td>0.402239</td>\n",
       "      <td>0.761696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663033</td>\n",
       "      <td>0.750054</td>\n",
       "      <td>0.781374</td>\n",
       "      <td>0.827452</td>\n",
       "      <td>0.616333</td>\n",
       "      <td>0.706000</td>\n",
       "      <td>0.758263</td>\n",
       "      <td>0.793197</td>\n",
       "      <td>0.888892</td>\n",
       "      <td>29.071717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23.077778</td>\n",
       "      <td>23.077778</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.667864</td>\n",
       "      <td>0.327543</td>\n",
       "      <td>0.081528</td>\n",
       "      <td>0.285531</td>\n",
       "      <td>0.818132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.629910</td>\n",
       "      <td>0.730102</td>\n",
       "      <td>0.753207</td>\n",
       "      <td>0.767546</td>\n",
       "      <td>0.509998</td>\n",
       "      <td>0.622755</td>\n",
       "      <td>0.667259</td>\n",
       "      <td>0.735256</td>\n",
       "      <td>0.767740</td>\n",
       "      <td>23.077778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   grid_size  scc_rank        tau       beta  gamma        RQ       JSD  \\\n",
       "0     2000.0       1.0  16.084848  16.084848    1.0  0.447073  0.433602   \n",
       "1     2000.0       2.0  25.075758  25.075758    1.0  0.498715  0.464845   \n",
       "2     3000.0       1.0  48.052525  48.052525    1.0  0.531685  0.324928   \n",
       "3     3000.0       2.0  17.083838  17.083838    1.0  0.660871  0.312646   \n",
       "4     5000.0       1.0  29.071717  29.071717    1.0  0.608960  0.407102   \n",
       "5     5000.0       2.0  23.077778  23.077778    1.0  0.667864  0.327543   \n",
       "\n",
       "        MSE      RMSE    RQ_net  ...  ndcg_100  ndcg_300  ndcg_500  ndcg_1000  \\\n",
       "0  0.212329  0.460792  0.673588  ...  0.518537  0.656073  0.726481   0.801077   \n",
       "1  0.223227  0.472469  0.754363  ...  0.807354  0.843784  0.883469   0.917787   \n",
       "2  0.276043  0.525398  0.744169  ...  0.570478  0.678075  0.730133   0.794098   \n",
       "3  0.046128  0.214775  0.773537  ...  0.742701  0.773690  0.787421   0.809069   \n",
       "4  0.161796  0.402239  0.761696  ...  0.663033  0.750054  0.781374   0.827452   \n",
       "5  0.081528  0.285531  0.818132  ...  0.629910  0.730102  0.753207   0.767546   \n",
       "\n",
       "    ndcg_1p   ndcg_5p  ndcg_10p  ndcg_20p  ndcg_50p    tau_opt  \n",
       "0  0.398057  0.550256  0.636761  0.735868  0.827745  16.084848  \n",
       "1  0.667798  0.780181  0.824938  0.840175  0.894838  25.075758  \n",
       "2  0.537202  0.669103  0.742679  0.806082  0.882797  48.052525  \n",
       "3  0.603978  0.734376  0.753121  0.775593  0.804727  17.083838  \n",
       "4  0.616333  0.706000  0.758263  0.793197  0.888892  29.071717  \n",
       "5  0.509998  0.622755  0.667259  0.735256  0.767740  23.077778  \n",
       "\n",
       "[6 rows x 26 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "with open('sis_results_list_100.pkl', 'rb') as f:\n",
    "    sis_results_list = pickle.load(f)   \n",
    "\n",
    "\n",
    "scc_df = pd.DataFrame(sis_results_list) #scc_df = pd.read_csv('scc_results.csv')\n",
    "\n",
    "best_tau_df = pd.read_csv('best_tau_by_grid_scc.csv')\n",
    "merged_rows = []\n",
    "\n",
    "for _, row in best_tau_df.iterrows():\n",
    "    g, r, tau_opt = row['grid_size'], row['scc_rank'], row['tau_opt']\n",
    "\n",
    "    \n",
    "    match = scc_df[\n",
    "        (scc_df['grid_size'] == g) &\n",
    "        (scc_df['scc_rank'] == r) &\n",
    "        (scc_df['tau'].round(6) == round(tau_opt, 6))  \n",
    "    ]\n",
    "\n",
    "    if match.empty:\n",
    "        print(f\"Warning: no match found for grid={g}, rank={r}, tau={tau_opt}\")\n",
    "        continue\n",
    "\n",
    "   \n",
    "    merged = match.mean(numeric_only=True).to_dict()\n",
    "    merged['grid_size'] = g\n",
    "    merged['scc_rank'] = r\n",
    "    merged['tau_opt'] = tau_opt\n",
    "\n",
    "    merged_rows.append(merged)\n",
    "\n",
    "\n",
    "final_df = pd.DataFrame(merged_rows)\n",
    "final_df = final_df.sort_values(['grid_size', 'scc_rank'])\n",
    "\n",
    "final_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0022160b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('sis_best_tau_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9d3506",
   "metadata": {},
   "outputs": [],
   "source": [
    "scc_eval_df = pd.read_csv('scc_eval_results.csv')\n",
    "pagerank_df= scc_eval_df[scc_eval_df['metric'] == 'pagerank_w']  # contains scc_eval_df[scc_eval_df['metric']=='degree'] results  \n",
    "ndcg_cols = ['ndcg_100', 'ndcg_300', 'ndcg_500', 'ndcg_1000']\n",
    "final_df= pd.read_csv('sis_best_tau_results.csv')\n",
    "\n",
    "\n",
    "combinations = final_df[['grid_size', 'scc_rank']].drop_duplicates().sort_values(['grid_size', 'scc_rank']).values\n",
    "\n",
    "n_plots = len(combinations)\n",
    "n_cols = 2\n",
    "n_rows = (n_plots + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 4 * n_rows), squeeze=False)\n",
    "\n",
    "for idx, (grid_size, scc_rank) in enumerate(combinations):\n",
    "    ax = axes[idx // n_cols, idx % n_cols]\n",
    "\n",
    "    \n",
    "\n",
    "    pr_row = pagerank_df[(pagerank_df['grid_size'] == grid_size) & (pagerank_df['scc_rank'] == scc_rank)]\n",
    "    if not pr_row.empty:\n",
    "        pr_vals = [pr_row.iloc[0][col] for col in ndcg_cols]\n",
    "        ax.plot(ndcg_cols, pr_vals, marker='s', label='PageRank baseline')\n",
    "\n",
    "    sis_row = final_df[(final_df['grid_size'] == grid_size) & (final_df['scc_rank'] == scc_rank)]\n",
    "    if not sis_row.empty:\n",
    "        sis_vals = [sis_row.iloc[0][col] for col in ndcg_cols]\n",
    "        ax.plot(ndcg_cols, sis_vals, marker='o', label='SIS model')\n",
    "\n",
    "    \n",
    "    ax.set_title(f\"grid_size={int(grid_size)}, scc_rank={int(scc_rank)}\")\n",
    "    ax.set_ylim(0.3, 1)\n",
    "    ax.set_ylabel(\"NDCG\")\n",
    "    ax.set_xlabel(\"Top-k\")\n",
    "    ax.legend()\n",
    "\n",
    "for idx in range(n_plots, n_rows * n_cols):\n",
    "    fig.delaxes(axes[idx // n_cols, idx % n_cols])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sis_results_ndcg_by_grid_scc.pdf', format='pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01147848",
   "metadata": {},
   "outputs": [],
   "source": [
    "colmns=['grid_size', 'scc_rank', 'tau_opt', 'RQ_net', 'JSD_net_norm',\n",
    "         'ndcg_100', 'ndcg_300','ndcg_500', 'ndcg_1000']\n",
    "final_df1 = final_df[colmns]\n",
    "final_df1\n",
    "final_df1.to_csv('ablation_study_sis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35afe916",
   "metadata": {},
   "outputs": [],
   "source": [
    "colmns=['grid_size', 'scc_rank',  'RQ', 'JSD',\n",
    "         'ndcg_100', 'ndcg_300','ndcg_500', 'ndcg_1000']\n",
    "pagerank_df= scc_eval_df[scc_eval_df['metric'] == 'pagerank_w']\n",
    "pagerank_df1= pagerank_df[colmns]\n",
    "pagerank_df1\n",
    "pagerank_df1.to_csv('ablation_study_pagerank.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb684c79",
   "metadata": {},
   "source": [
    "# Correlation: Node Centrality vs Fire risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "45f427d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cell', 'burning_count', 'degree', 'strength', 'in_degree',\n",
       "       'out_degree', 'in_strength', 'out_strength', 'v_true_sis', 'v_true_sf',\n",
       "       'v_true_net', 'v_true', 'burning_count_sf', 'burning_count_net'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid_size =5000 show the node metrics in node_metrics_list\n",
    "grid_size = 5000\n",
    "node_metrics = next((\n",
    "    rec['node_metrics'] for rec in node_metrics_list if rec['grid_size'] == grid_size\n",
    "), None)\n",
    "node_metrics.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801190f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import numpy as np\n",
    "\n",
    "n_rows = len(scc_node_result)\n",
    "metrics = [\n",
    "    'degree',\n",
    "    'strength',\n",
    "    'pagerank',\n",
    "    'eigenvector_centrality',\n",
    "    'betweenness_centrality',\n",
    "    'closeness_centrality',\n",
    "    'clustering_coefficient'\n",
    "   \n",
    "]\n",
    "spearman_results = []\n",
    "\n",
    "n_cols = len(metrics)\n",
    "# 6*? grid, width 30 inch, height 5 inch per row\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(30, 5 * n_rows), squeeze=False)\n",
    "\n",
    "for row_idx, rec in enumerate(scc_node_result):\n",
    "    node_metrics = rec['node_metrics']\n",
    "    v_true_sis = node_metrics['v_true_scc_sis'].values\n",
    "    grid_size = rec['grid_size']\n",
    "    scc_rank = rec['scc_rank']\n",
    "\n",
    "    for col_idx, metric in enumerate(metrics):\n",
    "        ax = axes[row_idx, col_idx]\n",
    "        v_metric = node_metrics[metric].values\n",
    "\n",
    "        sns.scatterplot(x=v_true_sis, y=v_metric, ax=ax)\n",
    "\n",
    "        ax.set_xscale('log')\n",
    "        if metric == 'strength':\n",
    "            ax.set_yscale('log')\n",
    "\n",
    "        ax.set_xlabel('Ground Truth (true network-driven fire probability)')\n",
    "        ax.set_ylabel(metric)\n",
    "        ax.set_title(f'Grid {grid_size}, Rank {scc_rank}\\n{metric}')\n",
    "\n",
    "       \n",
    "        try:\n",
    "          #  pearson_r, pearson_p = pearsonr(v_true_sis, v_metric)\n",
    "            spearman_r, spearman_p = spearmanr(v_true_sis, v_metric)\n",
    "\n",
    "           \n",
    "            x_pos = np.nanmax(v_true_sis)\n",
    "            y_pos = np.nanmax(v_metric)\n",
    "\n",
    "            ax.text(x_pos, y_pos,\n",
    "                   # f'Pearson r={pearson_r:.3f}, p={pearson_p:.3g}\\n'\n",
    "                    f'Spearman r={spearman_r:.3f}, p={spearman_p:.3g}',\n",
    "                    ha='right', va='top', fontsize=10,\n",
    "                    bbox=dict(facecolor='white', alpha=0.6, edgecolor='gray'))\n",
    "            spearman_results.append({\n",
    "                'grid_size': grid_size,\n",
    "                'scc_rank': scc_rank,\n",
    "                'metric': metric,\n",
    "                'spearman_r': spearman_r,\n",
    "                'spearman_p': spearman_p\n",
    "            })\n",
    "        except Exception as e:\n",
    "            ax.text(0.95, 0.95, f'Error:\\n{e}', transform=ax.transAxes,\n",
    "                    ha='right', va='top', fontsize=10,\n",
    "                    bbox=dict(facecolor='white', alpha=0.6, edgecolor='gray'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('/Users/mabelhu/Desktop/figure/scc_node.pdf', format='pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "266e0a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spearman = pd.DataFrame(spearman_results)\n",
    "df_spearman.to_csv('scc_node_spearman_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9681b701",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=scc_eval_df[scc_eval_df['metric'] == 'pagerank_w']\n",
    "columns=['grid_size', 'scc_rank', 'JSD', 'RQ']\n",
    "a = a[columns]\n",
    "a = a.sort_values(['grid_size', 'scc_rank'])\n",
    "a.to_csv('pagerank_eval.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fe09ef4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grid_size</th>\n",
       "      <th>scc_rank</th>\n",
       "      <th>rho</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.578604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.742680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.755673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.707319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.800925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.790563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   grid_size  scc_rank       rho\n",
       "0     2000.0       1.0  0.578604\n",
       "1     2000.0       2.0  0.742680\n",
       "2     3000.0       1.0  0.755673\n",
       "3     3000.0       2.0  0.707319\n",
       "4     5000.0       1.0  0.800925\n",
       "5     5000.0       2.0  0.790563"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_keep = [    'grid_size', 'scc_rank',   'rho']\n",
    "final_df[columns_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e226d728",
   "metadata": {},
   "source": [
    "# Method Validation: Spontaneous fire vs network driven fire "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33abb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('node_metrics_list.pkl', 'rb') as f:\n",
    "    node_metrics_list = pickle.load(f)\n",
    "   \n",
    "node_metrics_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2d6208",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapefile_path = \"/Users/mabelhu/Desktop/Code/DL_FIRE_SV-C2_576237/fire_archive_SV-C2_576237.shp\"\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "gdf = gdf[gdf['CONFIDENCE'].isin(['h', 'n'])]\n",
    "\n",
    "# Transform to meter-based\n",
    "if gdf.crs.to_string() == 'EPSG:4326':\n",
    "    gdf = gdf.to_crs(epsg=3857)\n",
    "\n",
    "# Convert to datetime\n",
    "gdf['ACQ_TIME'] = gdf['ACQ_TIME'].astype(str).str.zfill(4)\n",
    "gdf['acq_time'] = pd.to_datetime(\n",
    "    gdf['ACQ_DATE'].astype(str) + ' ' + gdf['ACQ_TIME'],\n",
    "    format='%Y-%m-%d %H%M'\n",
    ")\n",
    "gdf['acq_time'] = gdf['acq_time'].apply(pd.Timestamp)\n",
    "\n",
    "\n",
    "with open('node_metrics_list.pkl', 'rb') as f:\n",
    "    node_metrics_list = pickle.load(f)\n",
    "\n",
    "node_geo_list = []\n",
    "\n",
    "for item in node_metrics_list:\n",
    "    grid_size = item['grid_size']\n",
    "    node_metrics = item['node_metrics'][['cell', 'v_true_sis', 'v_true_sf']]\n",
    "\n",
    "    hex_grid = cu.create_hex_grid(gdf, grid_size)  # 应返回包含 'cell' 和 'geometry'\n",
    "\n",
    "    # 检查 'cell' 是否存在\n",
    "    if 'cell' not in hex_grid.columns:\n",
    "        raise ValueError(f\"'cell' column missing in hex_grid for grid_size={grid_size}\")\n",
    "\n",
    "    merged_df = node_metrics.merge(hex_grid[['cell', 'geometry']], on='cell', how='left')\n",
    "\n",
    "    for _, row in merged_df.iterrows():\n",
    "         node_geo_list.append({\n",
    "            'grid_size': grid_size,\n",
    "            'cell': row['cell'],\n",
    "            'v_true_sis': row['v_true_sis'],\n",
    "            'v_true_sf': row['v_true_sf'],\n",
    "            'geometry': row['geometry']\n",
    "        })\n",
    "\n",
    "node_geo_df = pd.DataFrame(node_geo_list)\n",
    "\n",
    "\n",
    "node_geo_df = gpd.GeoDataFrame(\n",
    "    node_geo_df,\n",
    "    geometry='geometry',\n",
    "    crs=gdf.crs  \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ec8db5",
   "metadata": {},
   "source": [
    "## compuet Spearman between v_true_sis and v_true_sf for each grid size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458a9ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compuet Spearman between v_true_sis and v_true_sf for each grid size\n",
    "spearman_results = []\n",
    "for grid_size in node_geo_df['grid_size'].unique(): \n",
    "    subset = node_geo_df[node_geo_df['grid_size'] == grid_size]\n",
    "    spearman_r, spearman_p = spearmanr(subset['v_true_sis'], subset['v_true_sf'])\n",
    "    spearman_results.append({\n",
    "        'grid_size': grid_size,\n",
    "        'spearman_r': spearman_r,\n",
    "        'spearman_p': spearman_p\n",
    "    })\n",
    "\n",
    "spearman_df = pd.DataFrame(spearman_results)\n",
    "print(spearman_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a136bdf",
   "metadata": {},
   "source": [
    "# Spatial Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0570bb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapefile_path = \"/Users/mabelhu/Desktop/Code/DL_FIRE_SV-C2_576237/fire_archive_SV-C2_576237.shp\"\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "gdf = gdf[gdf['CONFIDENCE'].isin(['h', 'n'])]\n",
    "\n",
    "# Transform to meter-based\n",
    "if gdf.crs.to_string() == 'EPSG:4326':\n",
    "    gdf = gdf.to_crs(epsg=3857)\n",
    "\n",
    "# Convert to datetime\n",
    "gdf['ACQ_TIME'] = gdf['ACQ_TIME'].astype(str).str.zfill(4)\n",
    "gdf['acq_time'] = pd.to_datetime(\n",
    "    gdf['ACQ_DATE'].astype(str) + ' ' + gdf['ACQ_TIME'],\n",
    "    format='%Y-%m-%d %H%M'\n",
    ")\n",
    "gdf['acq_time'] = gdf['acq_time'].apply(pd.Timestamp)\n",
    "\n",
    "with open('scc_node_result.pkl', 'rb') as f:\n",
    "    scc_node_result = pickle.load(f)\n",
    "\n",
    "scc_node_geo_list = []\n",
    "\n",
    "for item in scc_node_result:\n",
    "    grid_size = item['grid_size']\n",
    "    scc_rank = item['scc_rank']\n",
    "    node_metrics = item['node_metrics'][['cell', 'v_true_sis']]\n",
    "\n",
    "    hex_grid = cu.create_hex_grid(gdf, grid_size) \n",
    "\n",
    "    \n",
    "    if 'cell' not in hex_grid.columns:\n",
    "        raise ValueError(f\"'cell' column missing in hex_grid for grid_size={grid_size}\")\n",
    "\n",
    "    merged_df = node_metrics.merge(hex_grid[['cell', 'geometry']], on='cell', how='left')\n",
    "\n",
    "    for _, row in merged_df.iterrows():\n",
    "        scc_node_geo_list.append({\n",
    "            'grid_size': grid_size,\n",
    "            'scc_rank': scc_rank,\n",
    "            'cell': row['cell'],\n",
    "            'v_true_sis': row['v_true_sis'],\n",
    "            'geometry': row['geometry']\n",
    "        })\n",
    "\n",
    "scc_node_geo_df = pd.DataFrame(scc_node_geo_list)\n",
    "\n",
    "print(scc_node_geo_df.head())\n",
    "print(len(scc_node_geo_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3099e3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get v_sis with optial tau from sis_results_list_100.pkl and merge with sis_best_tau_results.csv\n",
    "with open('sis_results_list.pkl', 'rb') as f:\n",
    "    sis_results_list = pickle.load(f)\n",
    "sis_df = pd.DataFrame(sis_results_list)\n",
    "final_df= pd.read_csv('sis_best_tau_results.csv')\n",
    "\n",
    "final_df['tau_round'] = final_df['tau'].round(6)\n",
    "sis_df['tau_round'] = sis_df['tau'].round(6)\n",
    "\n",
    "merged_df = final_df.merge(\n",
    "    sis_df[['grid_size', 'scc_rank', 'tau_round','cc_node_list' ,'v_sis']],\n",
    "    on=['grid_size', 'scc_rank', 'tau_round'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(merged_df[['grid_size', 'scc_rank', 'tau_round','cc_node_list' ,'v_sis']])\n",
    "\n",
    "\n",
    "\n",
    "# match scc_node_geo_df with merged_df with v_sis\n",
    "scc_node_geo_df['v_sis'] = np.nan\n",
    "\n",
    "for _, row in merged_df.iterrows():\n",
    "    grid_size = row['grid_size']\n",
    "    scc_rank = row['scc_rank']\n",
    "    cc_nodes = row['cc_node_list']  # list of cell ids\n",
    "    v_sis_array = row['v_sis']      # array of same length\n",
    "\n",
    "    tmp_df = pd.DataFrame({\n",
    "        'cell': cc_nodes,\n",
    "        'v_sis': v_sis_array\n",
    "    })\n",
    "\n",
    "    mask = (\n",
    "        (scc_node_geo_df['grid_size'] == grid_size) &\n",
    "        (scc_node_geo_df['scc_rank'] == scc_rank)\n",
    "    )\n",
    "    scc_node_geo_df.loc[mask, 'v_sis'] = \\\n",
    "        scc_node_geo_df.loc[mask].merge(tmp_df, on='cell', how='left')['v_sis_y'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118a98ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'shapely.geometry.polygon.Polygon'>\n",
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(type(scc_node_geo_df.loc[0, 'geometry']))\n",
    "\n",
    "\n",
    "if isinstance(scc_node_geo_df.loc[0, 'geometry'], str):\n",
    "    from shapely import wkt\n",
    "    scc_node_geo_df['geometry'] = scc_node_geo_df['geometry'].apply(wkt.loads)\n",
    "\n",
    "\n",
    "scc_node_geo_gdf = gpd.GeoDataFrame(\n",
    "    scc_node_geo_df,\n",
    "    geometry='geometry',\n",
    "    crs=gdf.crs  \n",
    ")\n",
    "\n",
    "print(type(scc_node_geo_gdf))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb153c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.patheffects as pe\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# --- helper: load countries reliably across environments ---\n",
    "def load_countries():\n",
    "    try:\n",
    "        url = \"https://naturalearth.s3.amazonaws.com/110m_cultural/ne_110m_admin_0_countries.zip\"\n",
    "        return gpd.read_file(url)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            \"Could not load Natural Earth countries. \"\n",
    "            \"Install `geodatasets` (pip install geodatasets) or `cartopy`, \"\n",
    "            \"or ensure internet access for the direct URL.\"\n",
    "        ) from e\n",
    "\n",
    "# --- your original filtering ---\n",
    "filtered_gdf = node_geo_df[(node_geo_df['grid_size'] == 5000)]\n",
    "\n",
    "# --- load countries and match CRS ---\n",
    "countries = load_countries()\n",
    "\n",
    "# If your data has no CRS but is actually lon/lat WGS84, set it explicitly.\n",
    "if filtered_gdf.crs is None:\n",
    "    filtered_gdf = filtered_gdf.set_crs(4326)\n",
    "\n",
    "if countries.crs != filtered_gdf.crs:\n",
    "    countries = countries.to_crs(filtered_gdf.crs)\n",
    "\n",
    "# --- choose a country name column robustly ---\n",
    "name_col = next(col for col in ['NAME_EN', 'NAME', 'ADMIN', 'SOVEREIGNT'] if col in countries.columns)\n",
    "\n",
    "# --- get Colombia geometry and neighbors (land-touching) ---\n",
    "col_mask = countries[name_col].str.lower().eq('colombia')\n",
    "if not col_mask.any():\n",
    "    raise ValueError(\"Colombia not found in Natural Earth attributes.\")\n",
    "\n",
    "geom_series = countries.loc[col_mask, 'geometry']\n",
    "col_geom = geom_series.union_all() if hasattr(geom_series, \"union_all\") else geom_series.unary_union\n",
    "colombia = gpd.GeoDataFrame({name_col: ['Colombia'], 'geometry': [col_geom]}, crs=countries.crs)\n",
    "\n",
    "# neighbors that touch Colombia\n",
    "neighbors = countries[countries.touches(col_geom)].copy()\n",
    "\n",
    "# Some datasets include tiny maritime slivers; keep only neighbors with non-empty land border\n",
    "def shared_border(g):\n",
    "    inter = g.intersection(col_geom)\n",
    "    # We need a line-like shared border. If polygon intersection occurs due to topology quirks, use its boundary.\n",
    "    if inter.is_empty:\n",
    "        return None\n",
    "    # If intersection is an area (rare), take its boundary for labeling anchor\n",
    "    if inter.geom_type in ('Polygon', 'MultiPolygon'):\n",
    "        inter = inter.boundary\n",
    "    return inter\n",
    "\n",
    "neighbors['shared'] = neighbors['geometry'].apply(shared_border)\n",
    "neighbors = neighbors[neighbors['shared'].notnull() & (~neighbors['shared'].is_empty)]\n",
    "\n",
    "# --- map extent around Colombia only ---\n",
    "minx, miny, maxx, maxy = colombia.total_bounds\n",
    "padx = (maxx - minx) * 0.25\n",
    "pady = (maxy - miny) * 0.25\n",
    "extent = (minx - padx, maxx + padx, miny - pady, maxy + pady)\n",
    "\n",
    "# --- labeling helper: place text near shared border, nudged outward from Colombia ---\n",
    "cx, cy = col_geom.representative_point().coords[0]\n",
    "\n",
    "def label_neighbor_on_border(ax, border_geom, text):\n",
    "    # anchor near the middle of the shared border\n",
    "    try:\n",
    "        anchor = border_geom.representative_point()\n",
    "    except Exception:\n",
    "        anchor = border_geom.centroid\n",
    "    x, y = anchor.coords[0]\n",
    "\n",
    "    # offset direction away from Colombia centroid to push label到国境外侧\n",
    "    dx, dy = x - cx, y - cy\n",
    "    ha = 'left' if dx >= 0 else 'right'\n",
    "    va = 'bottom' if dy >= 0 else 'top'\n",
    "    ox = 3 if dx >= 0 else -3\n",
    "    oy = 3 if dy >= 0 else -3\n",
    "\n",
    "    ax.annotate(\n",
    "        text, (x, y), xytext=(ox, oy), textcoords='offset points',\n",
    "        ha=ha, va=va, fontsize=7, zorder=30,\n",
    "        path_effects=[pe.withStroke(linewidth=1, foreground=\"white\")]\n",
    "    )\n",
    "\n",
    "# --- draw ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8), dpi=300)\n",
    "\n",
    "# Norms (your settings)\n",
    "norm1 = colors.LogNorm(vmin=filtered_gdf['v_true_sis'].min() + 1e-4,\n",
    "                       vmax=filtered_gdf['v_true_sis'].max())\n",
    "norm2 = colors.LogNorm(vmin=filtered_gdf['v_true_sf'].min() + 1e-3,\n",
    "                       vmax=filtered_gdf['v_true_sf'].max())\n",
    "\n",
    "# ---- left plot ----\n",
    "ax0 = axes[0]\n",
    "filtered_gdf.plot(\n",
    "    column='v_true_sis',\n",
    "    cmap='OrRd',\n",
    "    linewidth=0.2,\n",
    "    edgecolor='black',\n",
    "    legend=True,\n",
    "    ax=ax0,\n",
    "    norm=norm1\n",
    ")\n",
    "xlim0, ylim0 = ax0.get_xlim(), ax0.get_ylim()\n",
    "minx, miny, maxx, maxy = colombia.total_bounds\n",
    "final_xlim = (min(xlim0[0], minx), max(xlim0[1], maxx))\n",
    "final_ylim = (min(ylim0[0], miny), max(ylim0[1], maxy))\n",
    "\n",
    "\n",
    "#colombia.boundary.plot(ax=ax0, color='dimgray', linewidth=0.8, zorder=15)\n",
    "countries.plot(ax=ax0, facecolor='none', edgecolor='dimgray', linewidth=0.6, zorder=10)\n",
    "\n",
    "# neighbor labels placed near the shared border, not drawing neighbor polygons\n",
    "for _, row in neighbors.iterrows():\n",
    "    label_neighbor_on_border(ax0, row['shared'], row[name_col])\n",
    "\n",
    "cx, cy = col_geom.representative_point().coords[0]\n",
    "ax0.annotate(\n",
    "    \"Colombia\", (cx, cy),\n",
    "    ha='center', va='center', fontsize=12, fontweight='bold',\n",
    "    path_effects=[pe.withStroke(linewidth=1, foreground=\"white\")]\n",
    ")\n",
    "\n",
    "ax0.set_xlim(xlim0)\n",
    "ax0.set_ylim(ylim0)\n",
    "ax0.set_title('v_true_sis', fontsize=12)\n",
    "ax0.set_axis_off()\n",
    "\n",
    "# ---- right plot ----\n",
    "ax1 = axes[1]\n",
    "filtered_gdf.plot(\n",
    "    column='v_true_sf',\n",
    "    cmap='OrRd',\n",
    "    linewidth=0.2,\n",
    "    edgecolor='black',\n",
    "    legend=True,\n",
    "    ax=ax1,\n",
    "    norm=norm2\n",
    ")\n",
    "\n",
    "#colombia.boundary.plot(ax=ax1, color='dimgray', linewidth=0.8, zorder=15)\n",
    "countries.plot(ax=ax1, facecolor='none', edgecolor='dimgray', linewidth=0.6, zorder=10)\n",
    "\n",
    "for _, row in neighbors.iterrows():\n",
    "    label_neighbor_on_border(ax1, row['shared'], row[name_col])\n",
    "cx, cy = col_geom.representative_point().coords[0]\n",
    "ax1.annotate(\n",
    "    \"Colombia\", (cx, cy),\n",
    "    ha='center', va='center', fontsize=12, fontweight='bold',\n",
    "    path_effects=[pe.withStroke(linewidth=1, foreground=\"white\")]\n",
    ")\n",
    "\n",
    "ax1.set_xlim(xlim0)\n",
    "ax1.set_ylim(ylim0)\n",
    "ax1.set_title('v_true_sf', fontsize=12)\n",
    "ax1.set_axis_off()\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"spatial_distribution_5000_1_compare.pdf\", format=\"pdf\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firenet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
