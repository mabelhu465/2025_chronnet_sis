{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cc5b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pickle\n",
    "import itertools\n",
    "from shapely.geometry import Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.patches import Polygon as MplPolygon\n",
    "import chronnet_utils as cu \n",
    "import warnings\n",
    "from scipy.linalg import eigvals\n",
    "from scipy import optimize\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from datetime import datetime\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "from sis_steady_state_and_eval import jsd_from_samples,nimfa_sis_steady_state_root_1,recognition_quality\n",
    "\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "from sklearn.metrics import ndcg_score, average_precision_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from compute_y_true import compute_y_true_metrics as compute_y_true\n",
    "current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f413688b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('chronnet_graph_list_20250615_141938.pkl', 'rb') as f:\n",
    "    chronnet_graph_list = pickle.load(f)\n",
    "\n",
    "with open('burning_result_list.pkl', 'rb') as f:\n",
    "    burning_result_list = pickle.load(f)\n",
    "\n",
    "all_y_true_sis = {\n",
    "    item['grid_size']: item['y_true_sis']\n",
    "    for item in burning_result_list\n",
    "}\n",
    "all_y_true_sf = {\n",
    "    item['grid_size']: item['y_true_sf']\n",
    "    for item in burning_result_list     \n",
    "}\n",
    "all_y_true_net = {\n",
    "    item['grid_size']: item['y_true_network']       \n",
    "    for item in burning_result_list\n",
    "}\n",
    "all_y_true = {\n",
    "    item['grid_size']: item['y_true']\n",
    "    for item in burning_result_list \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b2916f",
   "metadata": {},
   "source": [
    "# Tau Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70de290b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "grid_sizes = [2000, 3000, 5000]\n",
    "\n",
    "\n",
    "network_list = {\n",
    "    entry['grid_size']: entry['chronnet_pruned']\n",
    "    for entry in chronnet_graph_list\n",
    "}\n",
    "\n",
    "gamma = 1.0\n",
    "tau_c = 1.0\n",
    "tau_vals = np.linspace(1.1 * tau_c, 100 * tau_c, num=100)\n",
    "\n",
    "\n",
    "sis_results_list = []\n",
    "\n",
    "for grid_size in grid_sizes:\n",
    "    G = network_list.get(grid_size)\n",
    "\n",
    "    sccs = list(nx.strongly_connected_components(G))\n",
    "    top2_sccs = sorted(sccs, key=len, reverse=True)[:2]\n",
    "\n",
    "    \n",
    "    for rank, scc_nodes in enumerate(top2_sccs, start=1):\n",
    "        subG = G.subgraph(scc_nodes).copy()\n",
    "        # remove self-loops\n",
    "        subG.remove_edges_from(list(nx.selfloop_edges(subG)))\n",
    "        \n",
    "        cc_node_list = list(subG.nodes())\n",
    "        W = nx.to_numpy_array(subG, nodelist=cc_node_list, weight='weight', dtype=float)\n",
    "        lambda_max = np.max(np.abs(np.linalg.eigvals(W)))\n",
    "        W_norm = W / lambda_max\n",
    "\n",
    "        for tau in tau_vals:\n",
    "            beta = tau * gamma\n",
    "            print(f\"[grid {grid_size} | SCC {rank}] tau={tau:.3f}, beta={beta:.3f}\")\n",
    "\n",
    "            v_sis = nimfa_sis_steady_state_root_1(W_norm, beta, gamma)\n",
    "            if np.all(v_sis == 0):\n",
    "                print(f\"[grid {grid_size} | SCC {rank}] tau={tau:.3f} not converged, skip\")\n",
    "                continue\n",
    "            \n",
    "\n",
    "\n",
    "            y_true = np.array([all_y_true[grid_size].get(node, 0) for node in cc_node_list], dtype=float)\n",
    "            y_true_sf = np.array([all_y_true_sf[grid_size].get(node, 0) for node in cc_node_list], dtype=float)\n",
    "            y_true_sis = np.array([all_y_true_sis[grid_size].get(node, 0) for node in cc_node_list], dtype=float)\n",
    "            y_true_network = np.array([all_y_true_net[grid_size].get(node, 0) for node in cc_node_list], dtype=float)\n",
    "            v_hybrid = y_true_sf + (1 - y_true_sf) * v_sis\n",
    "            \n",
    "            \n",
    "            # hybrid\n",
    "            rq_val         = recognition_quality(v_hybrid, y_true)\n",
    "            jsd_val        = jsd_from_samples(v_hybrid, y_true, scale='minmax')\n",
    "            mse            = np.mean((v_hybrid - y_true) ** 2)\n",
    "            rmse           = np.sqrt(mse)\n",
    "\n",
    "            # net # jsd_val_net    = jsd_from_samples(v_sis, y_true_sis, scale='none')\n",
    "            rq_val_net     = recognition_quality(v_sis, y_true_sis)\n",
    "            mse_net        = np.mean((v_sis - y_true_sis) ** 2)\n",
    "            rmse_net       = np.sqrt(mse_net)\n",
    "            jsd_val_net_nm = jsd_from_samples(v_sis, y_true_sis, scale='minmax')\n",
    "            \n",
    "            # Compute Kendall and Spearman correlations\n",
    "            \n",
    "            rho=spearmanr(v_sis, y_true_sis).statistic\n",
    "            p_value=spearmanr(v_sis, y_true_sis).pvalue\n",
    "    \n",
    "            nd_50= ndcg_score([y_true_sis], [v_sis], k=50)\n",
    "            nd_100 = ndcg_score([y_true_sis], [v_sis], k=100)\n",
    "            nd_300 = ndcg_score([y_true_sis], [v_sis], k=300)\n",
    "            nd_500 = ndcg_score([y_true_sis], [v_sis], k=500)\n",
    "            nd_1000 = ndcg_score([y_true_sis], [v_sis], k=1000)\n",
    "\n",
    "\n",
    "            # compute the topk% ndcg\n",
    "            k1= int(len(v_sis) * 0.01)\n",
    "            k5= int(len(v_sis) * 0.05)\n",
    "            k10= int(len(v_sis) * 0.1)\n",
    "            k20= int(len(v_sis) * 0.2)\n",
    "            k50= int(len(v_sis) * 0.5)\n",
    "\n",
    "            nd_1p = ndcg_score([y_true_sis], [v_sis], k=k1)\n",
    "            nd_5p = ndcg_score([y_true_sis], [v_sis], k=k5)\n",
    "            nd_10p = ndcg_score([y_true_sis], [v_sis], k=k10)\n",
    "            nd_20p = ndcg_score([y_true_sis], [v_sis], k=k20)\n",
    "            nd_50p = ndcg_score([y_true_sis], [v_sis], k=k50)\n",
    "\n",
    "            \n",
    "            sis_results_list.append({\n",
    "                'grid_size': grid_size,\n",
    "                'scc_rank':      rank,\n",
    "                'tau':           tau,\n",
    "                'beta':          beta,\n",
    "                'gamma':         gamma,\n",
    "                'cc_node_list': cc_node_list,\n",
    "\n",
    "                'RQ':            rq_val,\n",
    "                'JSD':           jsd_val,\n",
    "                'MSE':           mse,\n",
    "                'RMSE':          rmse,\n",
    "                'RQ_net':        rq_val_net,\n",
    "                'MSE_net':       mse_net,\n",
    "                'RMSE_net':      rmse_net,\n",
    "                'JSD_net_norm':  jsd_val_net_nm,\n",
    "\n",
    "                'rho':           rho,\n",
    "                'p_value':       p_value,\n",
    "                'ndcg_50':       nd_50,\n",
    "                'ndcg_100':      nd_100,\n",
    "                'ndcg_300':      nd_300,\n",
    "                'ndcg_500':      nd_500,\n",
    "                'ndcg_1000':     nd_1000,\n",
    "                'ndcg_1p':       nd_1p,\n",
    "                'ndcg_5p':       nd_5p,\n",
    "                'ndcg_10p':      nd_10p,\n",
    "                'ndcg_20p':      nd_20p,\n",
    "                'ndcg_50p':      nd_50p,\n",
    "\n",
    "                'v_sis':         v_sis,\n",
    "                'v_hybrid':      v_hybrid,\n",
    "\n",
    "                'y_true':     y_true,\n",
    "                'y_true_sis': y_true_sis,\n",
    "                'y_true_sf':  y_true_sf,\n",
    "                'y_true_net': y_true_network\n",
    "               \n",
    "            })\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ab8e1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to a pickle file \n",
    "with open(f'sis_results_list_wo_selfloop.pkl', 'wb') as f:\n",
    "    pickle.dump(sis_results_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2206982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'chronnet_graph_list.pkl'\n",
    "with open(filename, 'rb') as f:\n",
    "    chronnet_graph_list = pickle.load(f)\n",
    "    \n",
    "grid_sizes = [2000, 3000, 5000]\n",
    "\n",
    "network_list = {\n",
    "    entry['grid_size']: entry['chronnet_pruned']\n",
    "    for entry in chronnet_graph_list\n",
    "}\n",
    "\n",
    "# node_metrics_list \n",
    "with open('node_metrics_list.pkl', 'rb') as f:\n",
    "    node_metrics_list = pickle.load(f)\n",
    "\n",
    "# chronnet_result_list\n",
    "with open('chronnet_result_list.pkl', 'rb') as f:\n",
    "    chronnet_result_list = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e5a2044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute eigenvector centrality for the top 2 SCCs in each grid size\n",
    "\n",
    "scc_node_result = []\n",
    "grid_sizes = [2000, 3000, 5000]\n",
    "\n",
    "for grid_size in grid_sizes:\n",
    "\n",
    "    G = network_list.get(grid_size)\n",
    "    sccs = list(nx.strongly_connected_components(G))\n",
    "    top2_sccs = sorted(sccs, key=len, reverse=True)[:2]\n",
    "\n",
    "    y_true_sis = all_y_true_sis.get(grid_size, {})\n",
    "    y_true_sf = all_y_true_sf.get(grid_size, {})\n",
    "    y_true_network = all_y_true_net.get(grid_size, {})      \n",
    "    y_true = all_y_true.get(grid_size, {})  \n",
    "\n",
    "    size_to_gdf = {\n",
    "    entry['grid_size']: entry['gdf_sjoined']\n",
    "    for entry in chronnet_result_list\n",
    "    }\n",
    "    \n",
    "    gdf_sjoined = size_to_gdf.get(grid_size, pd.DataFrame())\n",
    "\n",
    "    for rank, scc_nodes in enumerate(top2_sccs, start=1):\n",
    "        subG = G.subgraph(scc_nodes).copy()\n",
    "        # remove self-loops\n",
    "        subG.remove_edges_from(list(nx.selfloop_edges(subG)))\n",
    "        \n",
    "        cc_node_list = list(subG.nodes())\n",
    "\n",
    "        # Compute node metrics\n",
    "        in_deg_dict = dict(subG.in_degree())\n",
    "        out_deg_dict = dict(subG.out_degree())\n",
    "        in_str_dict = dict(subG.in_degree(weight='weight'))\n",
    "        out_str_dict = dict(subG.out_degree(weight='weight'))\n",
    "        deg_dict = dict(subG.degree())\n",
    "        str_dict = dict(subG.degree(weight='weight'))\n",
    "\n",
    "        ### !!!!! scc\n",
    "        scc_y_true_list= compute_y_true(\n",
    "            gdf_sjoined,\n",
    "            subG,\n",
    "            cell_col='cell',\n",
    "            time_col='time_group',\n",
    "            lag_hours=12\n",
    "        )\n",
    "        scc_y_true_dict=scc_y_true_list['y_true_dict']\n",
    "        scc_y_true_sf_dict=scc_y_true_list['y_true_sf_dict']\n",
    "        scc_y_true_network_dict=scc_y_true_list['y_true_network_dict']\n",
    "        scc_y_true_sis_dict=scc_y_true_list['y_true_sis_dict']\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "        subG_undir = subG.to_undirected() # ! only one edge is created with an arbitrary choice of which edge data to use.\n",
    "        ev_dict = nx.eigenvector_centrality(\n",
    "            subG,\n",
    "            max_iter=1000,\n",
    "            tol=1e-6\n",
    "            #,\n",
    "            #weight='weight'\n",
    "        )\n",
    "        ev_dict_w=nx.eigenvector_centrality(\n",
    "            subG,\n",
    "            max_iter=1000,\n",
    "            tol=1e-6,\n",
    "            weight='weight'\n",
    "        )\n",
    "        betweeness_dict = nx.betweenness_centrality(\n",
    "            subG,\n",
    "            normalized=True\n",
    "          #  weight='weight'\n",
    "        )\n",
    "        betweeness_dict_w = nx.betweenness_centrality(\n",
    "            subG,\n",
    "            normalized=True,\n",
    "            weight='weight'\n",
    "        )\n",
    "        closeness_dict = nx.closeness_centrality(\n",
    "            subG\n",
    "          #  distance='weight'\n",
    "        )\n",
    "        closeness_dict_w = nx.closeness_centrality(\n",
    "            subG,\n",
    "            distance='weight'\n",
    "        )\n",
    "        clustering_coeffs = nx.clustering(subG_undir)\n",
    "        clastering_coeffs_w = nx.clustering(subG_undir, weight='weight')   \n",
    "\n",
    "        # NetworkX's PageRank already uses a row‑stochastic transition matrix (out‑degree normalised).\n",
    "        pagerank_dict = nx.pagerank(subG, alpha=0.85, max_iter=100, tol=1e-6,weight=None)\n",
    "        pagerank_dict_w = nx.pagerank(subG, alpha=0.85, max_iter=100, tol=1e-6, weight='weight') \n",
    "\n",
    "        scc_node_metrics = pd.DataFrame({\n",
    "            'cell': cc_node_list,\n",
    "            'degree': [deg_dict[cell] for cell in cc_node_list],\n",
    "            'strength': [str_dict[cell] for cell in cc_node_list],\n",
    "            'in_degree': [in_deg_dict[cell] for cell in cc_node_list],\n",
    "            'out_degree': [out_deg_dict[cell] for cell in cc_node_list],\n",
    "            'in_strength': [in_str_dict[cell] for cell in cc_node_list],\n",
    "            'out_strength': [out_str_dict[cell] for cell in cc_node_list],\n",
    "            'clustering_coefficient': [clustering_coeffs[cell] for cell in cc_node_list],\n",
    "            'eigenvector_centrality': [ev_dict[cell] for cell in cc_node_list],\n",
    "            'betweenness_centrality': [betweeness_dict[cell] for cell in cc_node_list],\n",
    "            'closeness_centrality': [closeness_dict[cell] for cell in cc_node_list],\n",
    "            'eigenvector_centrality_w': [ev_dict_w[cell] for cell in cc_node_list],\n",
    "            'betweenness_centrality_w': [betweeness_dict_w[cell] for cell in cc_node_list],\n",
    "            'closeness_centrality_w': [closeness_dict_w[cell] for cell in cc_node_list],\n",
    "            'clustering_coefficient_w': [clastering_coeffs_w.get(cell, 0) for cell in cc_node_list],\n",
    "            'pagerank': [pagerank_dict[c] for c in cc_node_list],\n",
    "            'pagerank_w': [pagerank_dict_w[c] for c in cc_node_list],\n",
    "            \n",
    "            'v_true_sis': [y_true_sis.get(cell, 0) for cell in cc_node_list],\n",
    "            'v_true_net': [y_true_network.get(cell, 0) for cell in cc_node_list],\n",
    "            'v_true_sf': [y_true_sf.get(cell, 0) for cell in cc_node_list],\n",
    "            'v_true': [y_true.get(cell, 0) for cell in cc_node_list],\n",
    "\n",
    "            'v_true_scc': [scc_y_true_dict.get(cell, 0) for cell in cc_node_list],\n",
    "            'v_true_scc_net': [scc_y_true_network_dict.get(cell, 0) for cell in cc_node_list],\n",
    "            'v_true_scc_sf': [scc_y_true_sf_dict.get(cell, 0) for cell in cc_node_list],    \n",
    "            'v_true_scc_sis': [scc_y_true_sis_dict.get(cell, 0) for cell in cc_node_list]\n",
    "\n",
    "        })\n",
    "        \n",
    "        scc_node_result.append({\n",
    "            'grid_size': grid_size,\n",
    "            'scc_rank': rank,\n",
    "            'node_metrics': scc_node_metrics\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c966d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute JSD , recognition quality between node metrics and v_true_sis \n",
    "\n",
    "scc_eval_list = []\n",
    "\n",
    "\n",
    "for rec in scc_node_result:\n",
    "    node_metrics = rec['node_metrics']\n",
    "    v_true_sis = node_metrics['v_true_sis'].values\n",
    "    for metric in ['degree','strength','eigenvector_centrality','eigenvector_centrality_w', 'betweenness_centrality', 'closeness_centrality','pagerank', 'pagerank_w']:\n",
    "        v_metric = node_metrics[metric].values\n",
    "        jsd_val = jsd_from_samples(v_metric, v_true_sis, scale='minmax')\n",
    "        rq_val = recognition_quality(v_metric, v_true_sis)\n",
    "\n",
    "\n",
    "        rho = spearmanr(v_metric, v_true_sis).correlation\n",
    "        ken = kendalltau(v_metric, v_true_sis).correlation\n",
    "        ndcg_100 = ndcg_score([v_metric], [v_true_sis], k=100)\n",
    "        ndcg_300 = ndcg_score([v_metric], [v_true_sis], k=300)\n",
    "        ndcg_500 = ndcg_score([v_metric], [v_true_sis], k=500)\n",
    "        ndcg_1000= ndcg_score([v_metric], [v_true_sis], k=1000)\n",
    "        k1 = int(len(v_metric) * 0.01)\n",
    "        k5 = int(len(v_metric) * 0.05)\n",
    "        k10 = int(len(v_metric) * 0.1)\n",
    "        k20 = int(len(v_metric) * 0.2)\n",
    "        k50 = int(len(v_metric) * 0.5)  \n",
    "        nd_1p = ndcg_score([v_metric], [v_true_sis], k=k1)\n",
    "        nd_5p = ndcg_score([v_metric], [v_true_sis], k=k5)\n",
    "        nd_10p = ndcg_score([v_metric], [v_true_sis], k=k10)\n",
    "        nd_20p = ndcg_score([v_metric], [v_true_sis], k=k20)        \n",
    "        nd_50p = ndcg_score([v_metric], [v_true_sis], k=k50)\n",
    "\n",
    "\n",
    "        scc_eval_list.append({\n",
    "            'grid_size': rec['grid_size'],\n",
    "            'scc_rank': rec['scc_rank'],\n",
    "            'metric': metric,\n",
    "            'JSD': jsd_val,\n",
    "            'RQ': rq_val,\n",
    "            'rho': rho,\n",
    "            'ken': ken,\n",
    "            'ndcg_100': ndcg_100,\n",
    "            'ndcg_300': ndcg_300,\n",
    "            'ndcg_500': ndcg_500,\n",
    "            'ndcg_1000': ndcg_1000,\n",
    "            'ndcg_1p': nd_1p,\n",
    "            'ndcg_5p': nd_5p,\n",
    "            'ndcg_10p': nd_10p,\n",
    "            'ndcg_20p': nd_20p,\n",
    "            'ndcg_50p': nd_50p\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "scc_eval_df = pd.DataFrame(scc_eval_list)\n",
    "scc_eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec44a520",
   "metadata": {},
   "source": [
    "# Performance with the best tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55918203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "RQ_COL  = \"RQ_net\"\n",
    "JSD_COL = \"JSD_net_norm\"\n",
    "TAU_COL = \"tau\"\n",
    "\n",
    "def pick_tau_turning_point(df,\n",
    "                           rq_plateau_thresh=2e-3,   # |ΔRQ| below this ⇒ flat\n",
    "                           plateau_window=2,         # how many flat steps in a row\n",
    "                           rq_plateau_frac=0.95):    # fallback: ≥95 % of max RQ\n",
    "    \"\"\"\n",
    "    Given a (grid_size, scc_rank) sub‑DataFrame, return one row containing\n",
    "    tau_opt, RQ_net, JSD_net_norm that best trades off high‑and‑flat RQ vs\n",
    "    still‑low JSD.\n",
    "    \"\"\"\n",
    "    # 1. average repeated runs at the same τ\n",
    "    agg = (df.groupby(TAU_COL)\n",
    "             .agg({RQ_COL: \"mean\", JSD_COL: \"mean\"})\n",
    "             .sort_index()\n",
    "             .reset_index())\n",
    "\n",
    "    # If we only have a couple of τ values, just grab the “best” one.\n",
    "    if len(agg) < 3:\n",
    "        return agg.sort_values([RQ_COL, JSD_COL], ascending=[False, True]).iloc[0]\n",
    "\n",
    "    # 2. finite differences\n",
    "    agg[\"dJSD\"] = agg[JSD_COL].diff()\n",
    "    agg[\"dRQ\"]  = agg[RQ_COL].diff()\n",
    "\n",
    "    best = None\n",
    "    # 3. turning point: JSD just turned upward and RQ has been flat\n",
    "    for i in range(plateau_window, len(agg)):\n",
    "        jsd_turn = agg.loc[i - 1, \"dJSD\"] <= 0 and agg.loc[i, \"dJSD\"] > 0\n",
    "        rq_flat  = (agg.loc[i - plateau_window + 1 : i, \"dRQ\"]\n",
    "                      .abs()\n",
    "                      .lt(rq_plateau_thresh)\n",
    "                      .all())\n",
    "        if jsd_turn and rq_flat:\n",
    "            best = agg.loc[i, [TAU_COL, RQ_COL, JSD_COL]]\n",
    "            break\n",
    "            \n",
    "        if best is None:\n",
    "            max_rq = agg[RQ_COL].max()\n",
    "            plateau = agg[agg[RQ_COL] >= rq_plateau_frac * max_rq]\n",
    "            best = plateau.nsmallest(1, JSD_COL).iloc[0]\n",
    "\n",
    "    better = agg[\n",
    "        (agg[JSD_COL] < best[JSD_COL]) &\n",
    "        (agg[RQ_COL] > best[RQ_COL])\n",
    "    ]\n",
    "    if not better.empty:\n",
    "        best = better.sort_values(RQ_COL, ascending=False).iloc[0]\n",
    "\n",
    "    return best[[TAU_COL, RQ_COL, JSD_COL]]\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------ main driver\n",
    "# convert the list to a DataFrame and then to csv\n",
    "import pandas as pd\n",
    "import pickle\n",
    "with open('sis_results_list_wo_selfloop.pkl', 'rb') as f:\n",
    "    sis_results_list = pickle.load(f)\n",
    "df = pd.DataFrame(sis_results_list)\n",
    "\n",
    "\n",
    "best_rows = []\n",
    "for (gsize, rank), sub in df.groupby([\"grid_size\", \"scc_rank\"]):\n",
    "    best = pick_tau_turning_point(sub)\n",
    "    best_rows.append({\n",
    "        \"grid_size\":      gsize,\n",
    "        \"scc_rank\":       rank,\n",
    "        \"tau_opt\":        best[TAU_COL],\n",
    "        RQ_COL:           best[RQ_COL],\n",
    "        JSD_COL:          best[JSD_COL]\n",
    "    })\n",
    "\n",
    "results = pd.DataFrame(best_rows).sort_values([\"grid_size\", \"scc_rank\"])\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "# If you need it on disk:\n",
    "results.to_csv(\"best_tau_by_grid_scc_wo_selfloop.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f999c2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "with open('sis_results_list_wo_selfloop.pkl', 'rb') as f:\n",
    "    sis_results_list = pickle.load(f)   \n",
    "\n",
    "\n",
    "scc_df = pd.DataFrame(sis_results_list) #scc_df = pd.read_csv('scc_results.csv')\n",
    "\n",
    "best_tau_df = pd.read_csv('best_tau_by_grid_scc_wo_selfloop.csv')\n",
    "merged_rows = []\n",
    "\n",
    "for _, row in best_tau_df.iterrows():\n",
    "    g, r, tau_opt = row['grid_size'], row['scc_rank'], row['tau_opt']\n",
    "\n",
    "    \n",
    "    match = scc_df[\n",
    "        (scc_df['grid_size'] == g) &\n",
    "        (scc_df['scc_rank'] == r) &\n",
    "        (scc_df['tau'].round(6) == round(tau_opt, 6))  \n",
    "    ]\n",
    "\n",
    "    if match.empty:\n",
    "        print(f\"Warning: no match found for grid={g}, rank={r}, tau={tau_opt}\")\n",
    "        continue\n",
    "\n",
    "   \n",
    "    merged = match.mean(numeric_only=True).to_dict()\n",
    "    merged['grid_size'] = g\n",
    "    merged['scc_rank'] = r\n",
    "    merged['tau_opt'] = tau_opt\n",
    "\n",
    "    merged_rows.append(merged)\n",
    "\n",
    "\n",
    "final_df = pd.DataFrame(merged_rows)\n",
    "final_df = final_df.sort_values(['grid_size', 'scc_rank'])\n",
    "\n",
    "final_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38c5301",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "colmns=['grid_size', 'scc_rank', 'tau_opt', 'RQ_net', 'JSD_net_norm',\n",
    "         'ndcg_100', 'ndcg_300','ndcg_500', 'ndcg_1000']\n",
    "final_df1 = final_df[colmns]\n",
    "final_df1.to_csv('ablation_study_wo_selfloop.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firenet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
